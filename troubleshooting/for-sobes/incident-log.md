# Лог разбора инцидента

О себе (если параметры этой игры настраиваются под человека)
Есть опыт разбора инцидентов только для коробочного продукта.
Часто весь продукт на одном хосте (все 70 микросервисов),
иногда stateful части отделены.
Docker/docker-compose. Prometheus + Grafana.
О k8s пока только теоретические представления (завезли его к нам вчера, щупаем).
2 года руковожу devops командой, но в дежурствах участвую.
Наши фиксы это в основном - что упал
пайплайн, или какие-то косяки в наших инсталляторах у клиентов.
Или может когда клиент кластер развалил изменениями конфигов.
В рантайме крайне редко чем-то помогаем.
Книжку про SRE процессы в Google, которую рекомендовал
ваш Александр Поломодов - почитал.

* По системе можно спрашивать по ходу?
* Или важно все что можно спросить перед началом траблшутинга?

* Уточнить архитектуру (arc.md).
* Уточнить какие инструменты есть в наличие (tools-for-debug.md).
* Уточнить какие процессы уже построены в компании (processes.md).
* Выдохнули, избавились от стресса.
Убрали 1ю систему и подсознательные страхи, подключили вторую систему.
Пока не посмотрели данные - не строим догадки.

* Смотрим отчет.
Что ожидали, что в реале получили.

Если есть сценарии обработки инцидентов с разбитием по ролям - идем по ним.
Особенно круто если есть GUI, куда можно вводить всю инфу,
и все шаги, которые уже проверил, и шарить это на всех участников.

Допустим сценария и ролей нет.

# Тогда можно идти по примерно такому сценарию

* Отмечаем что взяли инцидент в работу.
* Оцениваем серьезность.
* Страшное уже произошло, или вот-вот произойдет?
* Думаем надо ли оповестить юзеров о проблеме,
* если что-то серьезное - оповещаем начальника.
* Думаем надо ли привлечь ещё коллег SREшников или может разработчиков.
* Если понимаем что портятся данные клиентов, или может произойти утечка
конфиденциальной инфы -
думаем может стоит вообще перевести систему в read-only maintenance mode.
Или вообще остановить всё.

* Понимаем симптомы, ищем возможность быстро купировать проблему, чтобы можно
было в более спокойной обстановке поразбираться.

* Если нет автоматических систем сбора логов - надо руками сохранить записи из журналов, для последующего анализа.

2 пути:
* Ищем сломанное звено в цепочке.
* Смотрим логи, что поменялось в системе.

* Увеличился ли трафик?

* Обязательно записываем гипотезы и результаты проверок.

* Также помечаем какие изменения внесли в систему, чтобы потом откатить
из отладочного состояния к продовому состоянию как было.


* Можно ли из какого-то мониторинга выдернуть проблемные запросы
и повторить их вручную или через какой-то наш инструмент.

* Если прям совсем большая цепочка - можно пойти бисекцией.

* Сделать воспроизводимый тест кейс.
Если ещё этот кейс вынести в тестовую среду, чтобы иметь большую
свободу в издевательствах над системой - вообще хорошо.

* Написать пост-мортем отчет (безобвинительно).

* Завести задачу на улучшение системы, чтобы проблема больше не повторилась.
* А если повторилась, чтобы мы мгновенно о ней узнали.


========

# Примерные шаги проверки:

Система не делает что надо, а что делает вместо этого?
Почему и где использует ресурсы.
Могут быть не те сущности. Либо сущности те, но направлены не туда.
Либо и то и другое.
Анализ последних изменений в системе - тоже хорошая отправная точка.





