# Вопросы про систему.

====================================================

Это может быть богатый сгенеренный автоматом отчет,
либо коллега просто сказал "система стала медленней".
Отчет должен содержать описание ожидаемого поведения,
реального поведени,
и если возможно - способа воспроизведения косяков.


Ожидаемое поведение, реальное поведение.


# План:
* Понять симптомы и временно купировать проблему.
*

* Сохранить записи из журналов, для последующего анализа.

* Если есть риск потери информации, - возможно лучше остановить всю систему. Или переключить её в read-only состояние.


*Дежурный подтверждает получение*
*Ранжирует проблему по приоритету*
*Думает надо ли привлечь ещё кого-то*
*Приступает к решению*

Вспомнили процедуры обработки инцидентов.
Или открыли доки.
Вспомнили какие у нас для них есть инструменты.

Как читать разные журналы:
Redis, Nginx, PG, Kafka, RabbitMQ, ClickHouse
Системные журналы линукс.
Журналы сетевых устройств.

=======

При данном инциденте какие риски потерять важные данные
клиентов, или утечки конфиденциальной инфы.
Не нужно ли остановить сервис целиком.



traceroute - научиться понимать до куда доходят пакеты.
А в каком месте - теряются.

Важно делить успешные и неуспешные запросы.
Неуспешные тоже медленно?
Ну это м.б. таймауты.
Но лучше какие-нибудь легкие запросы.

Может из региона юзера позапрашивать и посмотреть какие именно запросы тормозят.

Статика и динамика в мониторинге разделена ?

**дополнительный код** для обнаружения и отображения возможных причин.
* Информационные панели для каждой из возможных причин.

sentry

Например, временно отключив какой-то функционал, или перенаправив
трафик на другой экземпляр сервиса.

Написать пост-мортем отчет.

```
http_responses map:code 200:25 404:0 500:12
```

Сбои из-за часов?

Телефония. Алерты на телефоны.

Шаблоны языков для правил. Можно создавать библиотеки.
Можно создавать регрессионные и модульные тесты для правил.
И даже есть CI/CD для прохождения всех тестов и отправления
правил на экземпляры Borgmon.

Затраты на обслуживание должны расти медленее системы.

Соответственно по двум путям можно пойти:

# Менеджмент
* Выяснить что поменялось. Связать .

# Технина
* Копать от логов.

# Улучшения

Бот для траблшутинга

Запускается либо по алерту либо вручную.
Сам анализирует наиболее частые ошибки в логе.
Сам смотрит изменения в гите, и может релиз ноты, аудит.
Предоставляет инфу для отладки человекам.

Хелсчекинги системы.
На всех уровнях.
Сетевые связности.
Автоматический анализ логов от всех компонентов.

Сценарий (плейбук) обработки инцидента с автоматическими и ручными
блоками.



Можно раскидать критичные данные и не очень по хранилищам с разными характеристиками и разной стоимостью содержания.


Написать постмортем (безобвинительно).

Важно контролировать кол-во оповещений по одному инциденту.
Взаимосвязанные оповещения должны группироваться.
Чтобы они не смотрелись как разные инциденты.

DiRT (Disaster Recovery Training)

Есть ли частичная потеря пакетов (пинг).

Делаем пометки - пофиксить это в будущем.



Тулзы для дебага.
curl
ping

Воспроизводимый тест кейс.

Если есть мониторинг с запросами, то я их могу повторить.
Вопрос как повторить то, что идет как инфа для авторизации.
Т.е. как сделать чтобы мои curl запросы были как будто залогинины в систему.

Если ещё этот кейс вынести в тестовую среду, чтобы иметь большую
свободу в издевательствах над системой - вообще хорошо.

Если прям совсем большая цепочка - можно пойти бисекцией.


Система не делает что надо, а что делает вместо этого?
Почему и где использует ресурсы.

Могут быть не те сущности. Либо сущности те, но направлены не туда.
Либо и то и другое.




Анализ последних изменений в системе - тоже хорошая отправная точка.

Всегда записывайте какие идеи вы имели.
Какие тесты выполнили и какие результаты увидели.
Особенно когда имеете дело со сложными и затяжными кейами,
документация будет критической для вспоминания
что делал, чтобы не повторять то, что уже проверил.
Это также поможет подключиться другим людям.

Также помечайте какие изменения внесли в систему, чтобы потом откатить
если что.

Свели причины к одной.
Надо доказать что это та самая причина.
Допустим искусственно воспроизвести ситуацию.
В проде - может быть сложно. М.б. можно на тестовой среде.

Нашли все факторы - задокументь.
Полный постмортем и предложение как улучшить, чтобы такого не повторилось.

Увеличился ли трафик?

Задержки в БД м.б. связаны с плохой индексацией.

Важно контролировать расход памяти.
Тесты на потребление памяти.
Ожидаемы ли при разных действиях. И просто
типичные потребления памяти - в мониторинг.

Резервные ресурсы и копии - норм.

Важно тестить системы отката.

Если есть системы отката изменений. Деплоя. Конфигурации.
То можно их запустить с целью восстановления системы.

Система мониторинга может сама засрать сеть алертами.

Резервные средства коммуникации для SRE.
И консоли для отката изменений.
Инструменты SRE должны быть неподвержены отказам,
т.е. не юзать те же сети и мощности, что и основные продукты.

Возможность быстро перенаправить трафик.
Возможность быстро отключить средство автоматизации.

Есть ли регулярные треннинги.
Можно ли эскалировать к разрабам и другим SRE.
Кто ещё есть, к кому можно обратиться за помощью.
Может быть владельцы платформы, на которой мы хостимся.

Есть ли список постмортемов для изучения.
План действий должен быть на даже самые фантастические ситуации.

Если сбой сколько-то серьезен и уже случился, надо сообщить начальству и клиентам.

Расклады:
По процессам. Полная инструкция-протокол. Или даже сценарий обработки инцидента. С ролями разных людей. Чтобы знали что им делать, и не заходили на чужую территорию без согласования.
Это позволяет не угадывать поведение коллег.
По инструментам. Мониторинг, алертинг. Тестирование. Трейсинг.
Управление логированием.

Кто ответственнен за планирование.
Через него можно доставать больше ресурсов для траблшутинга.

Есть ли распределенные по часовым поясам команды.
Кому передавать эстафету.

Канареечные тесты.

=========
