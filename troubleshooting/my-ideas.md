*Дежурный подтверждает получение*
*Ранжирует проблему по приоритету*
*Думает надо ли привлечь ещё кого-то*
*Приступает к решению*

Есть ли более подробный план с архитектурой.
CDN, кэши. Средства мониторинга, дашборды.

По системе можно спрашивать по ходу?
Или важно все что можно спросить перед началом траблшутинга?
Уточнять детали вплоть до сетевых кабелей,
и какие рейды на каких дисках они юзают?
Вообще что-то о железе.

Выдохнули, избавились от стресса.
Убрали 1ю систему и подсознательные страхи, подключили вторую систему.
Пока не посмотрели данные - не строим догадки.

Вспомнили процедуры обработки инцидентов.
Или открыли доки.
Вспомнили какие у нас для них есть инструменты.


Есть ли официальный протокол обработки инцидента, если
понимаем что проблема требует ресерча и время неясно.
Прям веб приложуха.



Как читать разные журналы:
Redis, Nginx, PG, Kafka, RabbitMQ, ClickHouse
Системные журналы линукс.
Журналы сетевых устройств.

Ожидаемое поведение, реальное поведение.

Собирание инфы о системе.

Есть ли географическая распределенность.
Какую доступность решили поддерживать по аптайму и запросам.

=======

CDN.
Балансеры.

При данном инциденте какие риски потерять важные данные
клиентов, или утечки конфиденциальной инфы.
Не нужно ли остановить сервис целиком.



Настроены ли алерты (вообще на все).
Как происходит сборка логов.
Аудит действий с системой (изменения в ресурсах, в логах).
Инфраструктура как код. Гитопс.
Есть ли нагрузочные автотесты.
Есть ли стресс тестирование.
Давно ли работает система.
Есть ли резервная система, на которую можно быстро переключиться пока что. Сине - зеленые деплои.
Есть ли системы сбора данных на стороне клиентов.
Например, какой-нибудь ELMAH, с определением какие HTTP запросы
сбоят и отсылкой инфы на сервер наших логов.
Или можно определить, что JS работает долго у юзера.

Мониторинг у нас тупо усредненный или можно посмотреть по персентилям.

Есть ли у нас документация с разбором наиболее частых случаев
траблшутинга,
или наборами команд как что смотрится в какой системе (в гуях и в комманд лайне).
Чтобы не надо было наизусть это помнить.

Инцидент уже произошел, или вот-вот произойдет?
Держим ли мы резервные ресурсы?
Вообще где это все работает? Можно ли там быстро докупить ресурсов
на время?

Есть ли список наиболее приоритетных ошибок сервиса, влияющих
на надежность его работы.

Есть ли информационная панель с 4 мя золотыми сигналами,
или RED ом.

Есть ли ретро анализ, где видно что произошло ещё в то время как возрасли время запросов.

С качественными инструментами для анализа состояния "по факту"
Есть ли такие инструменты?
Автоанализ аномалий в запросах пользователей.

Есть ли мониторинг сетевых проблем (например, заломленный сетевой кабель).

Есть ли частичная потеря пакетов (пинг).

traceroute - научиться понимать до куда доходят пакеты.
А в каком месте - теряются.


Как давно выкатывались обновления и менялись конфигурации.
Есть ли аудит действий.

Важно делить успешные и неуспешные запросы.
Неуспешные тоже медленно?
Ну это м.б. таймауты.
Но лучше какие-нибудь легкие запросы.

Может из региона юзера позапрашивать и посмотреть какие именно запросы тормозят.

Статика и динамика в мониторинге разделена ?

**дополнительный код** для обнаружения и отображения возможных причин.
* Информационные панели для каждой из возможных причин.

sentry

Например, временно отключив какой-то функционал, или перенаправив
трафик на другой экземпляр сервиса.

Написать пост-мортем отчет.

Есть ли внутренние метрики сервисов, типа /varz:
```
http_responses map:code 200:25 404:0 500:12
```

Сбои из-за часов?

Телефония. Алерты на телефоны.

Шаблоны языков для правил. Можно создавать библиотеки.
Можно создавать регрессионные и модульные тесты для правил.
И даже есть CI/CD для прохождения всех тестов и отправления
правил на экземпляры Borgmon.

Затраты на обслуживание должны расти медленее системы.

Соответственно по двум путям можно пойти:

# Менеджмент
* Выяснить что поменялось. Связать .

# Технина
* Копать от логов.

# Улучшения

Бот для траблшутинга

Запускается либо по алерту либо вручную.
Сам анализирует наиболее частые ошибки в логе.
Сам смотрит изменения в гите, и может релиз ноты, аудит.
Предоставляет инфу для отладки человекам.

Хелсчекинги системы.
На всех уровнях.
Сетевые связности.
Автоматический анализ логов от всех компонентов.

Сценарий (плейбук) обработки инцидента с автоматическими и ручными
блоками.



Можно раскидать критичные данные и не очень по хранилищам с разными характеристиками и разной стоимостью содержания.


Написать постмортем (безобвинительно).

Важно контролировать кол-во оповещений по одному инциденту.
Взаимосвязанные оповещения должны группироваться.
Чтобы они не смотрелись как разные инциденты.

DiRT (Disaster Recovery Training)