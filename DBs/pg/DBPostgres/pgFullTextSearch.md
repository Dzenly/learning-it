Цикл статей по ускорению полнотекстового поиска.

http://shisaa.jp/postset/postgresql-full-text-search-part-1.html

Допустим, у нас есть phraseTable, содержащая тысячи VARCHAR строк и мы хотим найти фразу:

"An elephant a day keeps the dolphins at bay."

Но мы не помним строку точно. Мы помним, что есть слово elephant.

SELECT phrase FROM phraseTable WHERE phrase LIKE '%elephant%';

Мы просмотрим все индексы, попробует front and back wildcards и найдем.

Допустим юзер не помнит с большой ли буквы, тогда можно так:

SELECT phrase FROM phraseTable WHERE phrase ILIKE '%Elephant%';

Допустим, юзер думает, что Elephants.

SELECT phrase FROM phraseTable WHERE phrase ILIKE '%Elephants%';

Облом.

Если попробовать регулярные выражения, то будет нечитаемо и тормозно, не применяет индексацию.

## Полнотекстовый поиск.

Natural language - двусмысленный, неточный.

Полнотекстовый поиск - поиск документов или частей документов основанный на natural language searching.

Вы помните только, что там есть интересная фраза в тексте.
Что-то типа:
"The best elephants have a blue skin color."

Можно пробежать текст глазами и найти что-то похожее, в итоге выяснится, что фраза:
"The best Elephants bear a blue skin tone."

Мозг, используя внутренний словарь синонимов и толковый словарь считает многие слова похожими.

Чтобы быть быстрым движок поиска никогда не юзает сами документы. Это слишком медленно.

Сначала документы парсятся в списки слов. Этим спискам соответствует ссылка на документ.

Сначала документ фильтруется:

* Убираются stop words (her, him, the, also, each, was, we, after, been, they, would, up, from, only, you, while, и т.д., там могут быть тысячи слов), которые не несут особой информационной ценности для поиска.
* Всё приводится к lower case. В большинстве случаев полнотекстового поиска пользователю ищет в case-insensitive.
* Удаление синонимов (уменьшение кол-ва слов), применение тезауруса (словарь, с семантическими отношениями)(для того, чтобы сделать более компактными целые фразы), выделение основ слов.

Оставшиеся слова называют лексемами - минимальные, но несущие смысл слова. К лексемам привязаны два параметра:
* Указатель, на место в документе с этой лексемой.
* Вес лексемы (используется алфавит: A, B, C, D).

## tsvector (Text Search Vector)

Контейнер, в котором сохраняется результат парсинга.

Из такой строки:
"The big blue elephant jumped over the crippled blue dolphin."

PG (в простом случае, без тезауруса, весов, сжатия) сделает такой вектор:
'big':2 'blue':3,9 'crippl':8 'dolphin':10 'eleph':4 'jump':5
blue имеет два указателя, некоторые лексемы - обрезанные слова.

При парсинге слова категоризируются по секциям, типа:

* Слово
* URL
* int
* hword
* asciiword
* ...

Токены - категоризированные слова, промежуточная стадия.

Для каждой категории есть словарь. ПГ обращается к нему, чтобы получить лексему. И эти лексемы помещаются в вектор.

?? Если нет соответствие - слово отменяется ??
Исключение - "stop words", они отменяются, если соответствие найдено.


=========== 

Вместо *varchar* заюзаем *tsvector*.

```sql
CREATE DATABASE phrases;
\c phrases
CREATE TABLE phraseTable(phrase tsvector NUT NULL);
INSERT INTO phraseTable VALUES (to_tsvector('english','The big blue elephant jumped over the crippled blue dolphin.'));
// Можно использовать всякие тезаурусы и др. тулзы, но мы используем только встроенный механизм.
// to_tsvector (configuration (locale english by default),  - PG Full Text extension.
// configuration может включать словари, стоп ворд лист, и т.д.
SELECT phrase from phraseTable; // 'big':2 'blue':3,9 'crippl':8 'dolphin':10 'eleph':4 'jump':5
// Или так: 'big':2D 'blue':3D,9D 'crippl':8D 'dolphin':10D 'eleph':4D 'jump':5D
```

Чтобы избежать этого дефолтного поведери, можно когда упражняешься выставлять configuration в параметре.

Веса необязательный и дают средство для сортировки результатов. Некая метка для группировки лексем.
*Можно сделать побольше веса лексемах в заголовках и поменьше для простого текста.*

ПГ использует 4 метки: A, B, C, D. D - нижайший ранк. D используется по умолчанию.

Если все лексемы в tsvector - D, он может не отображаться.

Есть функция setweight.

```sql
UPDATE phraseTable SET phrase = setweight(phrase,'A'); //now: 'big':2A 'blue':3A,9A 'crippl':8A 'dolphin':10A 'eleph':4A 'jump':5A
```

```sql
update phraseTable set phrase=
setweight(to_tsvector('the big blue elephant'), 'A') ||
setweight(to_tsvector('jumped over the'), 'B') ||
setweight(to_tsvector('crippled blue dolphin.'), 'C');
// 'big':2A 'blue':3A,7C 'crippl':6C 'dolphin':8C 'eleph':4A 'jump':5B
```

Если документ имеет жесткую структуру, например title, body, footer. Можно сделать таблицу с тремя колонками tsvector и дальше можно распарсить и развесовать документ.

## tsquery

Теоретически, можно селектить данные из tsvector, однако нет способа фильтровать результаты через LIKE, ILIKE, да и синонимы тогда придется искать вручную.

*tsquery* - это тип данных, который используется при подготовке предиката поиска в *tsvector*, тогда будут использоваться специальные индексы для ускорения.

SELECT phrase FROM phraseTable WHERE phrase @@ to_tsquery('english','elephants');

to_tsquery - берет один аргумент - строку с токенами (не слова и не лексемы), соединенными операторами &, I, ! (можно использовать и скобки для группировки), и переводит его в лексемы для поиска.

SELECT phrase FROM phraseTable WHERE phrase @@ to_tsquery('elephants & blue');

Можно научить юзера пользоваться операторами и вводить правильные токены, но есть другая ф-я.

*plainto_tsquery* - преобразует произвольную строку в лексемы, как если бы они были соединены через &.

Обычно нужно добавлять обработку между вводом от юзером и формированием запроса к базе.
Убедиться, что это строка, и что она безопасна.

К тому же соединение лексем через & делает запрос сильно жестким.

Хорошо бы сделат запрос через | или позволить выбирать оператор в интерфейсе.
Можно ввести какие-то ключевые слова и обрабатывать запрос на их наличие.

## @@ оператор


text-search-matching operator (возвращает t or f)- специфичен для полнотекстового поиска. Позволяет матчит tsvector по результатам tsquery.

```sql
SELECT to_tsvector('The blue elephant.') @@ to_tsquery('blue & the');// Без обращения к таблицам.
SELECT 'The blue elephant.' :: VARCHAR @@ to_tsquery('blue & the'); // Works also with TEXT and VARCHAR
SELECT to_tsquery('elephants & blue'); // 'eleph' & 'blue'
SELECT to_tsquery('english','elephants & blue'); // the same
```

*to_tsquery* и *plainto_tsquery* используют конфигурацию того же типа, что и *to_tsvector*. И тоже есть первый опциональный аргумент - конфигурация.

# Конфигурация

http://shisaa.jp/postset/postgresql-full-text-search-part-2.html

*default_text_search_config*  *pg_catalog.english*

внутри psql можно посмотреть \dF - все конфиги, входящие в комплектацию.

\dF+ english - посмотреть подробней одну, там будут и категории токенов, для одной категории может определяться один или много словарей, они будут получать токет и пытаться вернуть лексему.

Если парсер встречает URL, он будет переводить его в *url* или *url_path* токен. ПГ заглянет в словарь, отмапленый на эту категорию чтобы попытаться и создать одну лексему, содержащую URL, указывающий на тот же путь. (?? не совсем понял).

Например:

* example.com
* example.com/index.html
* example.com/foo/../index.html

Типа эти URLs приводят к тому, что обрабатывается тот же самый документ, так что можно записать один вариант лексемы для вектора.
Тоже делается и для путей файлов, номеров версий, имен хостов, единиц измерения.

Есть 23 категории, распознаваемые парсером.

### Порядок обработки:

* Строка скармливается пг фул текст парсеру
* Парсер разбивает строку на токены конкретных типов
* Для каждой категории токен проходит через список словарей
* Словари обрабатываются в порядке от более специализированных до общих
* Как только нужная лексема найдена - поиск в словарях заканчивается
* Если стоп ворд лист вернул соответствие - токен игнорится




dictionary template - набор C функций.
dictionary - экземпляр темплейта с данными.

simple template - чекает на стоп ворды, если найдено - возвращает пустой массив, если нет - возвращает параметр приведенный к lowercase.

Инстантиация:
```
CREATE TEXT SEARCH DICTIONARY simple (
    template = simple,
    stopwords = english
);
```

Ф-и в темплейте должны быть мега-надежными, чтобы не поломать базу. Менять темплейт может только superuser.
Мы не указываем путь к фалу для stopwords, потому что есть стандартный english.

Cтоп ворд файл должен иметь .stop extension.

# Ranking and indexing

http://shisaa.jp/postset/postgresql-full-text-search-part-3.html

## ranking

Можно ранжировать по релевантности.

Документ, где больше искомых слов, и где они точнее - будет выше в поиске.

### Обычный ранкинг

Базируется на кол-ве вхождений слова.

ts_rank(tsvector, tsquery) - возвращает float - ранк документа.























