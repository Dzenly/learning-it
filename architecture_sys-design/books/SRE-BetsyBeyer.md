Site Reliability Engineering, edited by Betsy Beyer,
Chris Jones, Jennifer Petoff, and Niall Richard Murphy (O’Reilly). Copyright 2016
Google, Inc., 978-1-491-92912-4.

https://www.youtube.com/watch?v=q68S0wKy5vE

После появления ПО продукта много сил (40-90%) надо на поддержку и развитие.

# Цикл жизни ПО

* Создание
* Развертывание
* Функционирование
* Обновление
* Вывод из эксплуатации

====

Для разного ПО есть своё понятие "достаточно надежности".
АЭС - одно, интернет - магазин - другое.

====

Site в SRE - легаси.

====

SRE отличается от DevOps тем, что в нем упор идет на надежность.

====

Даже доскональная документация и жирные красные шрифты чего делать
не надо, не гарантируют отсутствие ошибок.
Люди все-равно рано или поздно сделают что-то запрещенное документацией.

====

Учет рисков - ключевой момент в SRE.

====

Надеяться - это плохая стратегия.

Operations (ops) - служба эксплуатации (развертывание, дежурство).

Без автоматизации OPS масштабируется только за счет найма админов.

Цели Ops и Dev противоречат друг другу.

Ops проверяют продукт.
Dev дробят продукт, чтобы проверять
нужно было только измененную часть.

Гугль в SRE команды набирает разработчиков, чтобы они
там по-максимуму все автоматизировали.

Это либо чистые разработчики без навыков админства (40-50%),
либо разработчики с навыками админства (UNIX, Network (OSI L1-L3)). Но всегда разработчики.

SRE разрабатывают приложения для решения сложных задач,
т.е. автоматизируют Ops.
Постоянно дорабатывают и улучшают свою автоматизацию.

**Гуглевский лимит - не больше 50% на операционку.**
Как раз чтобы было время на улучшение автоматизации.
В идеале стремятся к 100% автоматизации.

Как достигать этих < 50% операционки: перекладывать часть на Dev (весь избыток над 50%, вплоть до привлечения разработчиков к дежурствам и переадресации поступающих ошибок менеджерам разработки) или введение в SRE команду людей, на которых нет операционки.

При таком грамотном подходе, при масштабировании системы
SRE команда растет медленнее, чем система.

Ротации между разработчиками и SRE/

Из трудностей: т.к. в обе команды идут программисты -
конкуренция за людей с рынка труда.

**В менеджменте есть свои трудности**: например, остановить
выпуск версий до конца квартала, т.к. исчерпан лимит
времени недоступности сервиса.**

DevOps - обобщение SRE для широкого круга организаций, управленческих структур и персанала.
SRE - имплементация DevOps с расширениями.

# SRE команда отвечает за:

* Доступность
* Время отклика
* Производительность
* ?Эффективность?
* Управление изменениями
* Мониторинг
* Реагирование в аварийных и критических ситуациях
* Планирование производительности

=======

В организации должны быть систематизированы правила и принципы взаимодействия команд SRE с:
* Сопровождаемыми системами
* С командами разработчиков
* С командами тестировщиков
* С пользователями
* И т.д.

========

Когда разработчики участвуют в SRE (во время авралов, например),
они получают непосредственную обратную связь,
чтобы пилить системы, не требующие большой поддержки.

Если появляется больше двух проблем за 8-12 часовую смену,
- уже не хватает времени хорошо изучить проблему
и поправить архитектуру.

Но если в среднем меньше 1 события за дежурство - работа на месте дежурного - пустая трата времени.
? Почему, ведь можно в перерывах пилить свои задачи,
или добивать техдолг по архитектуре?

Нужно писать отчеты с анализом причин произошедшего (постмортемы)
для всех значимых инцидентов.
Особенно для событий, прошедших мимо мониторинга и алертинга.
Расследование должно установить все детали, и выработать план действий по устранению
или более эффективной обработки подобных инцидентов в будущем.
Чтобы ошибки не замалчивались - важно максимально избегать обвинений.

**Суммарный уровень/бюджет ошибок** (error budget)
Избавление от конфликта Dev vs Ops.

Наблюдение: 100% надежность необоснована в большинстве ситуаций (кроме, допустим: АЭС, кардиостимуляторов, систем безопасности автомобилей).

Разница в 99.999% и 100% теряется на фоне случайных факторов, и пользователь
не получает выхлопа от наших потраченных сил для добавления ещё одной 9ки.

# Вопросы к продукту при проектировании:

* Какой показатель доступности удовлетворит пользователей.
* Какие альтернативы имеют пользователи, неудовлетворенные доступностью продукта.
* Как изменится использование юзером продукта при разных уровнях доступности.

======

`ErrorBudget = 1 - PlannedAvailabilityTarget`

Этот бюджет можно тратить например, на
внедрение нового функционала и привлечение новых пользователей.

Можно освобождать error budget с помощью:
* Поэтапного развертывания (phased rollout).
* И экспериментального 1 процента (канареечные деплои ?)

В итоге Ops и Dev договариваются совместно на что потретить этот
error budget.
Т.е. баги и сбои не считаются чем-то плохим, ожидаемы, и входят
в тактические планы.
Исчезает страх, появляется лучший контроль.

======

# Мониторинг

* Почта неэффективна, если недостаточно очевидно что надо делать,
или если бывает что это просто некая инфа и делать с ней ничего не  надо.

* Срочные алерты (*alerts*) - нужно быстро отреагировать, т.к.
либо вот-вот произойдет, либо уже произошло что-то плохое.
* Запросы на действия (*tickets*) - Нужно вмешаться, но не обязательно сейчас, можно в течение нескольких дней.
* Журналирование (*logging*) - Пишется на всякий случай, для возможного последующего анализа для тикетов и алертов.

# Реакция на критические ситуации

MTTF - Mean Time To Faifure - Среднее время безотказной работы
MTTR - Mean Time To Repair - Среднее время восстановления

Ручные вмешательства - долго.
Продумывание всех деталей и **прописывание рекомендаций в инструкцию** - утраивает скорость восстановления системы, по сравнению с импровизациями.

Игры "Колесо неудачи" (Wheel of Misfortune)

============

# Управление изменениями

70% сбоев происходит из-за изменений в работающей системе.

## Нужно юзать автоматизацию для
* Поэтапного развертывания обновлений ПО;
* Быстрого и точного выявления проблем;
* Безопасного отката изменений при ошибках.

Типа убираем человеческий фактор: всякие усталости, невнимательности, замыленности, и т.д.

# Прогнозирование нагрузки и планирование производительности (ресурсов, мощностей)

* Закладывать небольшую избыточность?
* Нужно **как можно точнее** учитывать и естественный постепенный рост, и скачкообразные росты (новый функционал, сезонные эффекты, и т.д.). В том числе и за пределами срока, требуемого для ввода новых мощностей.
* Регулярное НТ, с отслеживанием насколько в каких местах грузится железо.

Команда SRE должна отвечать за все это планирование и за железо
(материально-техническое обеспечение (provisioning)).

# Мат-Тех обеспечение (Provisioning)

Оборудование дорогое. Надо делать все быстро и правильно, но только когда это
правда необходимо.

Наращивание производительности это: введение новых инстансов систем,
площадок размещения инфры, внесение изменений в конфиги, балансеры,
сетевые настройки, и т.д. Плюс проверка что все работает ожидаемо.

# Эфективное использование ресурсов

Коэффициент использования ресурсов влияет на стоимость сервиса.
При каком-то критичном перегрузе - может быть полный останов системы.

==============

# Оборудование

* Машина (комп, возможно виртуальная)
* Сервер - еденица ПО, реализующая сервис.

Сервера не привязаны к машинам. Ресурсы распределяются системой *Borg*.

* *Стойки* (десятки машин)
* *Ряды* стоек.
* Несколько рядов - *кластер*.
* Несколько кластеров - *ЦОД*.
* *Кампус* - Несколько близко расположенных ЦОДов.

Крутая сеть. Jupiter.
B4, OpenFlow.

==============

Для себя: видимо SRE все-таки должно норм владеть аппаратной частью,
чтобы понимать какие нагрузки какое оборудование держит.
Или даже писать софт, которое управляет конфигами оборудования.
Ну и надо понимать, что железо ломается, и не является
чем-то надежным.

==============

# ПО для оборудования

В крупных кластерах сбои оборудования случаются часто.
За год могут сломаться тысячи машин и жестких дисков.
**В общем, железо - расходник, не стоит рассчитывать на его надежность**

* В каждом кампусе есть команда, отвечающая за поддержку оборудования,
и инфраструктуру дата-центра.

## Управление машинами

*Borg* (он породил Кубер, похож на Apache Mesos)
borglet - kublet.

Borg запускает jobs. Это могут быть как постоянные сервисы,
так и процессы пакетной обработки вроде MapReduce.
Могут быть тысячи задач.
Работает примерно как кубер.

IP не статические.
Borg Naming Service, выделяется индекс для задачи.
`/bns/<cluster>/<user>/<job_name>/<job_index>`
И это уже можно преобразовать в `IP:Port`.

## Хранилище

Некая распределенная файловая система, похожая на HDFS или Lustre.

Disk(диски, флеши)->Colossus(FS, replication, cyphering, наследник Google File System)->BigTable (NoSQL, распределенная, разреженная, отказоустойчивая, многомерная БД, индексируется по ключам строк, столбцов и временным метком, каждое значение - произвольный массив байт, поддерживает репликацию)->Spanner (SQL для пользователей, где нужна целостность и согласованность).

Есть ещё всякие blobstore.

## Сеть

OpenFlow
Вместо умных и дорогих роутеров - глупые и дешевые коммутаторы + центральный контроллер, вычисляющий лучшие маршруты.

Bandwidth Enforcer (BwE) управляет доступеной полосой. Максимизирует
среднюю пропускную способность.
Централизованное управление трафиком решает проблемы,
которые плохо решаются распределенной маршрутизацией.

### Global Software Load Balancer, GSLB - три уровня:

* Географическая балансировка нагрузки для DNS запросов.
* Балансировка на уровне пользовательских интерфейсов (YouTobe, GMaps);
* Балансировка на уровне RPC.

Как данные для балансировки используются:
* Символьные имена, список BNS адресов.
* Производительность, доступная на каждой площадке (QPS, queries per second).

# Другое системное ПО

## Сервис блокировок (Chubby)
Предоставляет API, схожий с файловой системой.
Юзает Paxos для обращения к Consensus.
Играет роль при выборе мастера из реплик.
BNS использует Chubby

## Мониторинг и алертинг

Borgmon

* Оповещения о неотложных проблемах
* Информация для оценки как очередное ПО влияет на производительность
* Информация как потребляются ресурсы (допустим, с ростом кол-ва клиентов, или в какие-то сезонные моменты.). Все это - инфа для планирования нагрузки.

# Инфраструктура ПО

Сервисы умеют в многопоточность, и предоставляют HTTP интерфейс,
для сборка диагностической инфы и статистики.
Сервисы общаются по gRPC.

Часто локально вызывают RPCшные функции.
Потом легко отделить сервис.
GSLB - балансирует gRPC нагрузку.

Сервер получает RPC с фронта и отправляет их в бакэнд.
protobuf.

# Среда разработки

Общий репозиторий??
Можно делать МР в чьих-то файлах.
Ревью МР.
Параллельная компиляция.
CL - change list.
Тестирование кусков, на которые эти изменения **прямо или косвенно** могут повлиять.
**push-on-green** - Отправка в прод при успехе.

# Shakespeare: пример сервиса

* Читаем все найденные пока что тексты Шекспира и запихиваем их в Bigtable. Если найдем новые тексты - вызовем процедуру ещё раз.
* Фронт - для связи с пользователями.

## map-reduce
* Mapping: все тексты разбиваются на отдельные слова. Процесс идет параллельно.
* Shuffle - сортировка по словам.
* Reduce - индексы, кортежи вида (слово, список произведений).
Каждый кортеж - строка в Bigtable, ключ - слово.

## Жизненный цикл запроса

GSLB по символьному имени определяет какой IP возвратить пользователю.
Браузер соединяется с Reverse Proxy (Google Front End), он ищет сервис,в данном случае Shakespeare.
Дальше идет RPC на фронтэнд-сервер Shakespeare. И дальше запрос упаковывается в protobuf и идет на бакэнд-сервер. Через BNS, чтобы тоже балансировать нагрузку. Бакэнд запрашивает данные в Bigtable.
Опять protobuf, фронт-сервер, который создает HTML для ответа юзеру.

Все работает быстро и надежно.
Упреждающие методы восстановления при ошибках - (*постепенное отключение функций*.)

## Организация задач и данных

Допустим бэк может обработать 100 QPS.
Допустим, поняли, что пиковая нагрузка будет 3740 QPS.
Значит надо около 35 бэков.

Но во время обновления одна из задач всегда недоступна, и ещё можно учесть вероятность сбоев.
Поэтому добавим + 2 бэка.

Дальше можно поанализировать нагрузку по регионам, и раскидать
приложение по датацентрам этих регионов соответственно.
N+2 для каждого региона.
Для устойчивости - можно учесть межрегиональные перебросы.
Для дешевизны пренебречь N+2, оставив N+1 для малонагруженных регионов.

Bigtable дублируется в каждом регионе.
Консистентность прямо сию секунду не нужна.

=============

# Принципы

* Оценка рисков
* Управление рисками
* Использование лимита недоступности (error budget)

**SLO** Service Level Objectives

=============

# Приручаем риски

* Надежность не бесплатна. Каждая новая девятка может стоить в 100 раз больше предыдущей.
* Систему не имеет смысл делать сильно надежней, чем компоненты, через которые юзер работает с системой.

Цены:

* Машины. Резервные для переключения при сбоях или при обслуживании. А также хранящие контрольные суммы для обеспечения целостности.
* Упущенные возможности, когда разработчиков перебрасывают в SRE.

Строим графици стоимость, прибыль, уровень надежности, риски.

*Незапланированные отключения* как объективная метрика надежности.

`Availability = Uptime / (Uptime + Downtime).`

99.99 - 53 минуты простоя в год.

Но вообще **кол-во успешных запросов** за какой-то отрезок времени как метрика показательней.

`Availability = Successful requests / total requests.`

Допустим, Потеря 250 запросов из 2.5 млн запросов в день - 99.99%.
Есть разные типы запросов, они сильно важные, другие - нет. Так что это не 100% объективная метрика, но она достаточно хороша.

Запросы могут быть и всякие внутренние. Мож даже и не связанные с юзерскими запросами (а мож и связанные).

Доступность гугль замеряет раз в квартал. А производительность
еженедельно или даже ежедневно.

## Рискоустойчивость сервисов

Иногда это критически важно. И тогда понятие рисков включено в характеристики продукта.

Риски оценить не всегда просто.
Инфраструктурные сервисы (системы хранения или уровень кэширования для HTTP) могут не иметь определенного владельца.

### Пользовательские сервисы

Часто имеют команды и продакт овнера (менеджера).
Если команды нет, требования к доступности выполняют
люди, создающие систему, даже если они об этом и не знают.

#### Факторы при оценке рисков

* Какого уровня доступности надо достичь.
* Как различные типы сбоев влияют на сервис.
* Как мы можем менять стоимость сервиса, чтобы позиционировать его на кривой зависимости рисков.
* Какие другие показатели сервиса нужно иметь в виду.

#### Целевой уровень доступности

* Какого уровня доступности будут ожидать клиенты.
* Связан ли сервис непосредственно с прибылью (нашей или пользователей).
* Сервис платный или бесплатный?
* *Какие есть конкуренты, какой уровень сервиса у них.*
* *Сервис для предприятий или для конечных пользователей?*

Google Apps for Work - 99.9%.
+ Более высокий уровень внутренней доступности.
+ Контракт со штрафами, если не выполнили уровень.

А вот когда Google купил YouTube, он поставил уровень надежности поменьше, **чтобы развиваться быстрее**.

#### Типы сбоев

* Что лучше несколько мелких сбоев или один большой?
* Есть полные, есть частичные сбои. Например, в профайле все работает, кроме фоток.
* **Есть сбои, когда лучше полностью отключить сервис**. Например, когда компрометируются данные пользователей.
* Некоторые сервисы используются в определенное время дня, и их можно отключать в другое время.

#### Стоимость

* Что нам стоит добавить ещё 9-ку?
* Сколько мы получим прибыли из-за этого?

Если нет однозначной зависимости прибыли от надежности.
Можно взять за основу надежность сервисов, через которые используют твой сервис.
Например, надежность интернет провайдера пользователя.
Разные инет провайдеры это 99 - 99.99%.

#### Другие показатели сервисов

* Задержка (иногда она даже желательна, например, для показа рекламы)


## Определение рискоустойчивости инфраструктурных сервисов

Отличаются от требований к потребительским продуктам.
Т.к. имеют несколько клиентов, чьи потребности часто не совпадают.

Где-то важна *надежность*, где-то *пропускная способность*,
где-то *латентность*.

* Какое состояние очереди запросов желательно для пользователей каждого типа?

### Типы отказов

* Те, которым нужна низкая латентность - хотя видеть очереди пустыми.
* Те, кому нужна пропускная способность - хотят, чтобы очереди были не пустыми.

### Стоимость

Можно разделить инфру и ввести несколько независимых уровней обслуживания.

Короткие очереди - за счет доп. ресурсов.
Конкуренция между потребителями уменьшается, очереди разгружаются.

Но в 2-10 раз дешевле настроить кластер на максимальную пропускную способность.

Можно раскидать критичные данные и не очень по хранилищам с разными характеристиками и разной стоимостью содержания.

На одном и том же железе и ПО можно настраивать разные классы систем,
за счет конфигов.

### Пример: фронтенд-архитектура

Фронтэнды (реверс прокси, балансировщики) делают максимально надежными. Т.к. тут есть риск полностью потеряь запрос.


## Обоснования критерия суммарного уровня ошибок (errors budget)

Природа конфликта Dev vs Ops.
Разработчиков оценивают по тому, сколько фич они запилили.
Ops оценивают по надежности систем.

* *Устойчивость* к сбоям. Слабая устойчивость а нежданчикам -
будет дырявое в плане безопасности и нестабильное приложение.
Если устойчивость будет сильной, приложение никто не захочет
использовать. *Почему??* Дорого?? Долго, конкуренты опередят?
* *Тестирование*. Мало тестишь - ошибки, разного уровня серьезности.
Долго тестишь - теряешь рынок.
* *Частота релизов*. Каждое обновление у клиента - риск.
* *Продолжительность тестирования и размер выборки*.
Канареечные версии. Как долго ждать и как много юзеров переводить на канарейку.

Во многих компаниях и продуктах это все выбирается необъективно.
Какие-то исторически сложившиеся компромисы.
Лучше стараться оценивать объективно на разных количественных метриках.

## Формируем error budget

Команды его совместно определяют на квартал.
Основываясь на SLO.

Гугль делает так:

* Менеджер продукта определяет SLO. Задавая время uptime в течение квартала.
* Система мониторинга (нейтральная третья сторона) замеряет текущий uptime.
* Разница и есть запас (бюджет) на оставшуюся часть квартала.
* Если есть запас - можно продолжать выпуск обновлений.

Например, если какая-то проблема съедает 0.0002 аптайма, а целевой 0.001. То бюджет ошибок эта проблема выгребает на 20%.

## Преимущества

Поиск баланса между инновациями и надежностью. Вместе. И Ops и Dev.
Если SLO не достигается - приостанавливаем выпуски, дорабатываем продукт. Можно даже откатить релиз.

И тут разработчики подумают стоит ли выпускать релиз, который
может съесть весь бюджет и остановить разработку.
Это если у SRE команды есть право останавливать релизы.

Если обе команды понимают что дело в железе - они сообща
решают приостановить релизы.

SLO не обязан быть константой, при каких-то условиях его можно подправлять.

=================

## Целевой уровень качества обслуживания

### SLI - service level indicators (показатели)
Четко определенное числовое значение конкретной характеристики
обслуживания.
* Время отклика
* Уровень и частота ошибок (процент успешных запросов от всех, даже сформированных некорректно)
* Throughput (QPS)
Берется окно измерения, данные усредняются, рисуются процентили.

Не всегда можно напрямую измерить что-то.
Мы часто знаем время обработки запроса на сервере, но вот задержку у клиента померить сложнее.

* *Доступность* (availability) Доля успешных запросов из тех, что сформированы корректно. Также называется *yield* (выработка, отдача, урожай).
* *Durability* - долговечность. Вероятность что данные не потеряются
за какое-то время.

Две девятки - 99. Пять девяток - 99.999.
Google Compute Engine - 3.5 девяток. Т.е. 99.95%

### SLO

SLI <= Target
lower_bound <= SLI <= upper_bound

I.e. для Shakespeare - средняя латентность поиска должна быть меньше 0.1 сек.

Значит надо пилить фронт с низкой латентностью, или улучшать оборудование.

Выбор SLO сложен. Например, QPS определяется субъективными
ощущениями пользователей.

Есть статья Speed Matters, где проведены исследования как я понял по тому, как скорость влияет на удержании юзеров.

И латентность и QPS могут ухудшаться после определенного предела нагрузки.

Юзеры могут считать систему и более надежной чем она есть и менее надежной.

Например, кто-то там в своих сервисах завязался на Chubby, считая его 100% надежным, и был неприятно удивлен во время их обслуживании.
Иногда чтобы приучать юзеров создают **искусственные сбои**.

### SLA

Явный или нет контракт с юзерами, **включающий последствия**
при соответствии или несоответствии SLO.

**Можно создавать и без SRE**

Google Search - сервис без SLA.

=================

## Показатели на практике

Не надо в SLI пихать все, что есть в мониторинге.
Надо выбрать действительно важные параметры.

* Для *пользовательских сервисов* обычно важны *доступность*, *латентность*, *пропускная способность*.
* Для *систем хранения* - *латентность*, *доступность*, *долговечность*.
* Для *систем обработки больших объемов данных* (Big Data Systems) (pipeline processing) -
*thoughput* and *end-to-end latency* (from ingestion to completion).
Некоторые конвееры могут иметь целевые показаели латентности на индивидуальных этапах).
* *correctness* - For all systems. Правильные ли данные возвращены,
был ли корректен выполненный анализ.
Правда он часто - свойство данных а не системы, и не входит в зону ответственности SRE.

### Сбор показателей

Borgmon или Prometheus.
Журналы приложений.
Иногда данные можно собирать на  **стороне клиента**.
Например скорость обработки JS или метрики по загрузке CDN.

### Агрегация

Часто сырые первичные данные агрегируются. Это нужно делать осторожно.

Если у вас есть выброс в обработке запроса, который сгладился
т.к. вы усредняете по минуте, вы проглядите проблему.

**Лучше смотреть распределения** по персентилям.

50-й персентиль - медиана, типичные ситуации.

Чем выше разброс значений, тем больше влияет на пользователя
хвост распределения. Эффект усугубляется при высоких нагрузках
из-за поведения очередей.

**SRE Можно ориентироваться на какой-нибудь 99.9 процентиль.**

О статических заблуждениях: среднее для IT не то же самое как для физики. Среднее арифметическое может сильно отличаться от медианы.
И закон распределения не факто что будет нормальным.
Это всё нужно учитывать в автоматизации. Например если автотулза
рестартит сервис, она должна полагаться не на интуитивные распределения, а на фактические.

### Стандартизация показателей (SLI)

Можно стандартизовать некие общие показатели, чтобы не думать над
ними каждый раз заново.
Тогда останется думать только над специфическими показателями.

* Интервал агрегации: среднее значение по минуте.
* Регион агрегирования: все задачи в кластере.
* Частота измерений: каждые 10 сек.
* Какие запросы включаем в выборку: GET, полученные мониторингом методом черного ящика.
* Способ получения данных: Получены путем наблюдения. Измерения на стороне сервера.
* Задержка при доступе к данным: Время получения последнего байта.

## Целевые показатели на практике

В первую очередь отталкиваться от того, что важно клиентам.
А не от того, что мы можем измерить.
Т.е. лучше ставить цели, а потом думать как замерить показатели для них.

### Определение целей

Нужна инфа как измерять SLO и условия их действительности.
Например:

* 99% (в среднем за минуту) вызовов RPC Get будут завершены
менее чем за 0.1 сек. (По всем бэкам).
* 99% RPC Get будут завершены менее чем за 0.1 сек. Остальные параметры эта строка берет из SLI defaults.

Если важна форма кривых производительности, можно указать несколько
целевых значений:

* 90% RPC Get будут завершены менее чем за 1ms.
* 99% RPC Get - 10ms
* 99.9% RPC Get - 100ms

Если есть юзеры с разной загруженностью (вроде конвейера обработки
массивов данных), для которых важна throughput и интерактивный клиент, для которого важно время отклика, то для каждого вида
нагрузки лучше задать свои SLI

* 95% RPC Set, для которых важна throughtput, < 1 sec.
* 99% RPC Set, для которых важна latency и которые имеют нагрузку
меньше 1Кб, - будут завершены < 10ms.

SLO не должны соблюдаться 100% времени.
Лучше установить уровень допускаемого несоответствия SLO,
и регулярно его трекать (ежедневно, еженедельно),
тогда можно увидеть, что что-то потихоньку начинает идти не так,
и заранее принимать меры.

+ Ежемесячный или ежеквартальный отчет по использованию error budget.

### Выбор целевых показателей

Что надо учитывать:

* Интересы продукта и бизнеса.
* Ограничения кадрового состава.
* Сроками выхода на рынок.
* Доступность аппаратных средств.
* Финансы.

#### Уроки гугла

* Не выбирать цель, основываясь на текущей производительности.
Лучше безотносительно текущих подумать какие они должны быть.
Возможно где-то можно сэкономить.
* Не усложняйте агрегацию. Тут можно потерять важные данные.
* Не гонитесь за абсолютом. Good enough for users.
* Минимизируйте кол-во SLO до тех, которых достаточно
для получения полной информации о характеристиках системы.
Обосновывайте необходимость SLO. Если среди команд нет согласия,
что данный SLO нужен. То может лучше его и не вводить. Да и возможно
SRE тут не нужно.
* Идеал может подождать. Лучше сначала ориентироваться
на реально доступный уровень, чем тужиться потом.

SLO должны стать главным ориентиром при приоритезации задач SRE
и даже разработчиков.
Чем лучше продуманы SLO, - тем эффективней тратится время Dev и SRE,
и тем более доволен клиент.

### SLO как элемент управления

* Измеряйте SLI
* Сравнивайте с SLO, решайте нужно ли что-то предпринять.
* Решайте что конкретнопредпринять.
* Действуйте.

Например, идет рост времени отклика. Можно временно повысить ресурсы,
и разбираться почему отклик растет.

### Опубликованные SLO формируют ожидания пользователей

Соответственно пользователи имеют больше инфы чтобы выбрать или
не выбрать ваш сервис.
Например, если доступность низкая - это норм для хранения не сильно нужных архивов.

#### Тактики

* Запас прочности. Более жесткие внутренние SLO.
Тогда будет больше времени на разбор проблем и больше маневров занести в продукт что-то тяжелое (но м.б. более дешевое).
* Избегать перевыполнения. Пример с Chubby. Не приучайте юзеров
завязываться на крутые SLO. Время от времени имитируйте отключения.
**Или можно искусственно ограничивать QPS до SLO-шных.**
Или проектировать систему так, чтобы она работала с одинаковой скоростью при разных нагрузках. Т.е. допустим довыделяла себе
только сколько надо ресурсов при больших нагрузках, и возвращала
их обратно при снижении нагрузок.

### Соглашения на практике

Когда думаем над SLO с овнерами и юристами, роль SRE - донести реалистичность выполнения разных SLO.
Чем шире круг пользователей, тем труднее изменить или удалить SLA,
которые оказались трудновыполнимыми или необоснованными.

===========================

# Избавляемся от рутины

*Если при выполнении стандартных операций нужен человек, у вас есть ошибка*
Карл Гейссер.

*Служебная нагрузка* - обсуждения, определение целей и приоритетов,
отчеты, бумажная работа для кадровой службы. Найм людей. Ревью кода.
Рефакторинги. Прохождение обучающих курсов.

*Грязная работа* - очистка конфигурации системы оповещения,
удаление мусора. В перспективе имеет ценность.Но это не рутина. От меня: ??Правда?? По мне так это можно автоматизировать, а значит рутина.
Разбирание с устаревшим кодом - грязная работа, но ведет к улучшениям,
не рутина.

*Рутина* - имеющая один или несколько из след. признаков:
* ручная (Ручной запуск автоматизированного сценария - рутина)
* однообразная (т.е. встречающаяся не первый раз),
* поддающаяся автоматизации (если в процессе автоматизированного решения, человек должен что-то решить это не совсем рутина, но возможно можно переделать систему, что участие человек не понадобится)
* операционка по поддержке работающего сервиса (реакция на алерты например).
* Результаты не имеют ценности в перспективе (если сервис после ваших действий не улучшился - скорее всего рутина).
* Трудоемкость растет линейно по мере роста сервиса. Хорошо спроектированный
сервис должен масштабироваться на порядок без нужды в человеке.

## Почему важно минимизировать рутину

Если с рутиной не бороться, она съест.
В гугле на рутину отводится не больше 50% (в среднем за несколько кварталов или год) и жестко выдерживаются эти рамки.
Если нарушаем - разбираемся.

### Определяем кол-во рутины

* SRE одну неделю цикла проводит как дежурный первой очереди
и одну неделю - как дежурный второй очереди.
* Допустим в ротации участвует 6 человек.
Значит 33% на рутину.
Если 8 - 25%.
Я так понимаю верхняя граница 50%.
* Главный источник рутины - поступающие запросы (например, несрочные сообщения, связанные с работой сервиса).
* Следующий источник (я так понимаю не по приоритету, а по объему), -
срочные ответы по запросу с последующими релизами и пушами (что блин?? в общем какие-то дела как-то связанные с релизами и пушами).
* Ежеквартальные опросы показывают что на рутину гугль SRE тратят
примерно 33%, что лучше целевых 50%.
Но это в среднем по больнице. На самом деле есть и 0%, а есть и 80%.
И тут надо менеджерам балансировать.

## Что такое инженерная работа

Проектирование универсальных и общих решений, реализация автоматизации.
Позволяет не увеличивать персонал при масштабировании сервисов.

* *Разработка ПО*. Написание авто-сценариев.
Создание инструментов и фреймворков.
Добавление функционала для масштабирования и надежности,
повышение надежности инфраструктурного кода.
* *Разработка систем* Конфигурирование, документирование, рефакторинг для последующего удобства работы с настройкой.
Настройка мониторинга, обновлений, балансировщиков нагрузки, параметров ОС.
Консультация разработчиков по архитектуре и передаче в эксплуатацию.
* *Рутина* по обслуживанию сервисов.
* *Доп. служебная нагрузка* (см. выше).

## Всегда ли рутина вредна

* Однообразная работа может кому-то быть и норм.
* Полностью избежать рутины невозможно.

### Почему избыток рутины - плохо
* *Застой в карьере*. Нужно участвовать в проработках и реализации автоматизации.
* *Снижение производительности*. Выгорание, скука, недовольство.
* *Если делаешь рутину, то не делаешь инженерные задачи*, соответственно проекты перестают нормально масштабироваться.
* *Путаница*. Если ты транслируешь, что рутина - норм, остальные не понимают зачем мы её автоматизируем тогда.
* *Замедляется прогресс*. Рутина растет. Меньше времени на нерутинные задачи. Выпуск релизов, и т.д.
* *Нежелательные прецедент* Если вам рутина норм - на вас её вешают все больше, снимая с себя и расслабляясь (особенно касается разработчики). Привыкают, считают нормой.
* *Потери личного состава* - Если команду перегрузить рутиной - люди начнут искать места поинтереснее.
* *Подрыв веры* - на собесе сказали, что будет много автоматизации,
в реале - рутина, что негативно влияет на боевой дух и производительность.

======================

# Мониторинг распределенных систем

В индустрии типа есть проблемы с терминами.
* *Мониторинг (наблюдение)* Сбор, обработка, агрегирование и отображение
в реальном времени количественных показателей системы, например общее
число и тип запросов, кол-во и тип ошибок, время обработки запросов, аптайм сервера.
* *Наблюдение методом белого ящика* - Использование показателей,
доступных внутри системы. E.g. Java Virtual Machine Profiling Interface, обработчики HTTP по внутренней статистике.
* *Наблюдение методом черного ящика* - Наблюдение и проверка (тестирования) поведения видимого извне, как это видит юзер.
* *Информационная панель (панель управления) (Dashboard)* Приложение (обычно веб), предоставляющее сводку показателей сервиса. Может иметь фильтры, селекторы
и т.д. Но заранее её конфигурируют так, чтобы показывать наиболее
важные для пользователей сведения.
Может также отображать важные для команды сведения, типа длины очереди тикетов,
списка наиболее приоритетных ошибок, имени дежурного инженера,
инфа о последних установках ПО.
* *Оповещение (алерт)* - Сообщения, на которые должен обратить внимание
человек. Направляются в очередь тикетов, имейл (как правило неэффективно), пейджер, телефон (вызовы, экстренные оповещения).
* *Основная причина* (Root Cause) - Дефект в ПО, или человеческая ошибка, после
исправления которой аналогичное событие не произойдет.
Может быть *несколько основных причин*.
Недостаточная автоматизация,
программный сбой из-за некорректных входных данных,
недостаточно качественное тестирование сценария конфигурации.
И т.д.
* *Узел или машина (Node)* - Один экземпляр ядра (?приложения?), на физическом или виртуальном сервере или в контейнере.
На одной ноде м.б. стоит мониторить несколько сервисов.
Серверы могут быть:
* Либо связаны друг с другом, например, кэш и веб сервер.
* Несвязанные сервисы, шарящие хардварь: например, репу кода, или мастер конфигурационной системы
типа Puppet или Chef.
* *Push* - Любые изменения в софте или конфигурации.

## Зачем мониторить

* *Анализ и экстраполяция тенденций* Как быстро растет БД, QPS, кол-во пользователей.
* *Сравнение с предыдущими версиями или экспериментальными группами (канарейками?)* Как работают новые БД или версии старых БД. Улучшаются ли характеристики с добавлением доп. узлов. Сравнение работы сервиса
с прошлой неделей.
* *Оповещение* Что-то либо уже сломалось. Либо вот-вот сломается. Кому-то
нужно заняться.
* *Создание информационных панелей* (**4 golden signals** см. дальше).
* *Ретро анализ (например, отладка)* Допустим мы поняли что возрасло врема отклика, что ещё случилось в это время.
* *Бизнес-аналитика*
* *Анализ брешей в защите*

Сообщения должны быть по делу.
Тикеты "что-то немного странно выглядит" - не должны отвлекать инженеров. Если речь не о мега-критичных вещах.

Вызов инженера - очень дорог.
Если вызовы часто - работает "волки волки".
Не должно быть шумов, чтобы сигнал/шум, было большим.

## Реальные задачи для мониторинга

* Мониторинг за крупным приложением - сложная задача.
Команда может выделять до 20% ресурсов на создание и обслуживание
системы мониторинга.
При обобщении и централизации инфры для мониторинга расходы снижаются.
Но все-равно должен быть хотя бы один наблюдатель.
Наблюдать всякие дашборды может быть интересно и забавно,
но как только там надо искать ошибки - желающих мало.

* В Google создают простые и быстрые системы мониторинга.
С качественными инструментами для анализа состояния "по факту"
Гугль избегает "магических", которые самостоятельно исследуют
пороговые значения или причинно-следственные связи. ?Почему?
Исключение - правила, обнаруживающие нежданчики в запросах
конечных пользователей.
Долгосрочные закономерности - норм поддаются автоматическому анализу.

Если есть очень стабильные части системы - в гугле могут вводить разные правила.
Например: "Если дата-центр пуст, то не сообщать о его задержке отклика".
Некоторые команды вводят и сложные иерархии зависимостей.

Система мониторинга и алертинга должны быть просты для понимания всеми
в команде.
Оповещения тоже, простые, понятные, и только в случаях реальных (возможно грядущих)сбоев.

## Симптомы и причины

Система мониторинга должна отвечать на два вопроса:
* Что сломалось (симптом)?
* Почему (причина)?

### Примеры:

Симптом / причина
* Сервис возвращает 500 или 404. Сервер БД отказывает в соединении.
* Медленные ответы.  Процы перегружены или заломлен сетевой кабель.
* Пользователи не получаеют часть от CDN. Возможно их адреса попали в черный список IP.
* Личные данные видны всему миру. Что-то случилось с ACL, возможно при последнем обновлении.

## Черный и белый ящики

Черный ящик применяется редко, но метко.
Видит реально возоникшие а не спрогнозированные проблемы.

МБЯ зависит от возможности иссследовать систему изнутри (журналы, интерфейсы HTTP). Позволяет прогнозировать проблемы,
замаскированные рестартами сбои, и т.д.

Симптом и причина - понятия относительные. Медленная база - симптом для БД, но причина для сбоев в бэке и фронте.
Так что белый ящик может выдавать и симптомы и причины.

При сборке телеметрии необходим белый ящик.
Например веб-сервер медленно работает с БД. Может дело в БД,
а может в связи с БД.

Черный ящик - как источник алертов - норм, т.к. нет ложных тревог.

## 4 золотых сигнала (показателя)

LTES

* *Время отклика (Latency)* Важно делить успешные и неуспешные запросы.
* *Трафик (Traffic)* Нагрузка на систему в высокоуровневых &L7? единицах. HTTP RPS **отдельно для статического и динамического контента**.
Для потоковых видео/аудио - скорость передачи или кол-во параллельных сессий. Для k/v storages - transactions per second или кол-во возвращенных значений.
* *Уровень ошибок (Errors)* Кол-во (или частота) неуспешных запросов, либо явных (500),
либо неявных (200, но с битым контентом внутри) либо не соответствующих требованиям
Например SLO говорят, что запрос должен занимать не больше секунды. - Все что больше - ошибки.
?Коды ответов не могут отразить весь спектр? Могут понадобиться внутренние протоколы.
Если мониторить лишь балансировщик, будешь получать слабоинформативный 500,
а вот если мониторишь все можешь раскопать откуда он в итоге взялся.
* *Степень загруженности (Насыщение)(Saturation)* Для ограниченных ресурсов (память, IO диска, и т.д.) Многие системы теряют в производительности до того, как израсходуют 100% ресурсов.
Можно дополнить высокоуровневыми оченками нагрузки. Например, что будет при удвоении
трафика. Сколько % из доп. трафика обработается. А может вообще все тормознется,
и даже половина от общего трафика не обработается.
Все усложняется, если в зависимости от параметров - время выполнения запросов - разное. И приходится использовать косвенные показатели с известным верхним пределом.
Например *процент загрузки процессора* или *сети*.
Первый индикатор *насыщения* - увеличение времени отклика.
**Измерение 99го процентиля времени отклика за последнюю минуту** - говорит, что
намечается перегрузка.
Важно обращать внимание на прогнозы типа: "Похоже, ваша БД заполнит весь диск через 4 часа".

## Забота о "хвостах"

Средние оценки часто малопоказательны.
99% процентиль задержки у нас - может быть средней задержкой у какого-то клиента.
Гистограма распределения запросов в логарифмической шкале.

## Уровень детализации

* Контроль ЦПУ с периодичностью в минуту замажет пики.
* С другой стороны если заявлены 9 часов даунтайма в год, то пинговать сервер раз в 30 сек на предмет возврата 200 или проверять заполненность HDD может быть излишним.

Частые измерения - много инфы, дорого хранить.
Можно оптимизировать, накапливая данные с интервалом 1 сек, и сбрасывая на сервер хранения с интервалом в минуту.

* Записывать загрузку ЦПУ каждую секунду.
* Сделать динамическую гистограмму по 5%, где границы нагрузки для каждого столбца зависят от текущей нагрузки.
* Раз в минуту собирать итоговые данные.

## Просто, но не проще чем надо

Уровни сложности:
* Оповещения о достижении порогов в задержке отклика и разных процентилей для всех видов показателей.
* **дополнительный код** для обнаружения и отображения возможных причин.
* Информационные панели для каждой из возможных причин.

Типа если сильно усложнять систему мониторинга - её станет трудней поддерживать.

* Правила, определяющие реально аварийные ситуации должны быть простыми,
предсказуемыми, надежными.
* Если что-то используется слишком редко (реже раза в квартал) и редкими людьми, - от него м.б. можно избавиться.
* Если что-то собирается но не отражается на панелях и не используется в оповещениях и автоматизациях, - это кандидаты на удаление.

Заманчиво объединить мониторинг с кучей всего (профилирование, отладка, отслеживание исключений, анализ журналов, и т.д.). Но система станет сложной и менее стабильной.
Лучше иметь небольшие блоки с четким назначением.

### Правила при созданий правил мониторинга:

* Позволяет ли правило выявить *не обнаруживаемое иными средствами состояние*,
которое требует быстрой реакции и последствия которого уже заметны (или 100% скоро будут заметны) пользователю.
* Смогу ли я проигнорить это оповещение, зная что оно не критическое.
В каком случае и почему я проигнорю это сообщение? Как я могу избежать игнора.
Dzenly: Я так понимаю тут речь о том, что некритические оповещения тоже требуют реакции. Возможно лучше автоматом сразу тикеты заводить на них, чтобы не проигнорить.
* Понятно ли из оповещения, что уже есть негативное влияние на пользователей.
Отфильтрованы ли сценарии когда мониторинг реагирует на наши собственные действия (запуск нагрузочных тестов, отвод трафика), и пользователи не страдают.
* Могу ли я предпринять что-то в ответ на оповещение. Нужно ли реагировать
немедленно (например, ночью), или может подождать до утра.
**Можно ли безопасно автоматизировать действие. Будет ли это решение постоянным или временным.**
* Приходит ли это оповещение другим инженерам? Значит либо твое либо их - возможно зря отвлекают людей от работы.

### Позиция гугля по экстренным оповещениям на пейджеры (и мобильные телефоны видимо)

* Каждый раз я должен отреагировать мгновенно, но осмысленно. Если это будет больше нескольких раз за день - переутомлюсь.
* На каждое оповещение надо среагировать.
* Если среагировать может и автомат (т.е. не надо думать) - это не экстренное оповещение. ?странно? А что если автомата ещё нет ?
* Экстренное оповещение и вмешательство человека необходимы при появлении новой проблемы или ранее не встречавшегося события.
Dzenly: я так понимаю, это потому, что у них дежурный сразу старается фиксить систему,
чтобы подобной ошибки не было больше никогда.

Если все 4 пункта соблюдены - не важно черным или белым ящиком сгенерировано оповещение.
Лучше потратить время на поиск симптомов, чем на поиск причин. Причины
стоит искать самые явные и неизбежные. ?? Пока не понял почему.

## Долгосрочное наблюдение

По меняется, нагрузка меняется, SLO меняются.
М.б. на данный момент оповещение Н бывает редко, и ответ на него автоматизировать сложно, в дальнейшем оно может начать появляться чаще, и возможно
заслужит вложений в автоматизацию (??сделанную наспех??).
Тогда-то надо будет попытаться полечить root-cause, и если это не получится
- полностью по уму автоматизировать решение.

* Важно учитывать долгосрочные переспективы.
Каждый экстра-алерт отвлекает человек от улучшения системы.
Зачастую лучше отказаться от высоких SLO сейчас, чтобы улучшить
эти показатели в будущем.

### Пример с Bigtable: когда оповещений слишком много

У Гугля есть SLO для внутренней инфры.
Раньше SLO для Bigtable основывалось на синтетических средних показателей
для "хороших" клиентов. Из-за проблем в сервисе и в слое хранения даннных,
- худшие 5% запросов были значительнее медленнее средних.

При приближении к SLO - рассылались имейл оповещения, а при их превышении - экстра-алерты на пейджер.
Тратилось много времени на проверку, чтобы найти действительно требующие
срочных мер и из-за человеческого фактора, некоторые важные сообщения игнорились.
Многие оповещения были known-issues.

Было три этапа решения:
* Усилия по улучшению производительности BigTable.
* Взяли в SLO значение 75го процентиля времени отклика.
* Отключили имейл оповещения.

Т.е. переключились с тактических микрорешений на стратегическое.
Сервис улучшился.

### Пример с Gmail: предсказуемые ответы человека

У людей была проблема, и они спорили стоит ли делать костыль,
было мнение что из-за костыля не решится root-cause.
Такие автокостыли - красные флаги для менеджеров и других людей расставляющих приоритеты.

Важно смотреть загрузку команды оповещениями и тех. долг.

## Итоги

Оповещения важны.
Имейл - неэффективен.
Информационная панель важна.
Можно её соединить с журналом, чтобы анализировать динамику показателей.

Оповещения о симптомах и приближающихся проблемах, позволяют оптимизировать дежурства. Важно устанавливать достижимые SLO. И сделать возможность
быстрой диагностики через системы мониторинга.


# Эволюция автоматизации в Гугле

* Если у вас сильные автоматизаторы - осторожно, к силе нужна грамотная точка приложения. Автоматизация не в тех местах, может принести больше проблем, чем решить их.
* В идеале лучше создавать высокоуровневую автономную систему, не нуждающуюся
ни в автоматизации ни в ручной работе.

## Польза автоматизации

### Постоянство

Масштабирование - достаточная мотивация для автоматизации, но есть и другие мотивации:
* Управление группой компьютеров.
* Поддержка разного софта.
* Создание пользовательских учеток.
* Проверка создания бакапов,
* Устранение сбоев при отказах сервера,
* Манипуляция с данными (например изменение resolv.conf вышестоящих DNS и информации о DNS зонах).

Человек каждый раз выполняет одно и то же немножко по-разному, соответственно
человеческий фактор, ошибки, недостаточная надежность.
Основная польза автоматизации - воспроизводимость действий.

### Платформа

Если качественно проектируешь и имплементируешь автоматизацию, это
можно переиспользовать в других системах или для других задач,
плюс это приносит большой опыт во всем, что автоматизировал.

Лучше исправлять ошибку, чем обходить её каждый раз ручными действиями.
Нередко обучить людей выполнить какие-то действия сложнее, чем написать
автоматизацию.

Автоматизации не надо спать или долго соображать.

Платформа может экспортировать показатели производительности и предоставлять
детальную инфу о процессе.

### Быстрое восстановление

Автоматизация снижает MTTR (Mean Time To Repair) после ошибок.
Время людей освобождается для решения других задач.

Dzenly: Я так понимаю тут речь идет об автоматизации обнаружения проблем,
а не только об автоматизации их решения или закостыливания.

Чем раньше найдешь проблему (желательно вообще в прототипе), тем дешевле её поправить.

### Быстродействие

Автоматизация реагирует быстрее человека. В некоторых сервисах гугля столько
автоматизации, что никакой штат SRE не вывез бы её вручную.
Главное делать её грамотно, чтобы она не навредила системе.

### Экономия времени

Сравниваем усилия, сколько надо на написание автоматизации и сколько она сэкономит
человеко-часов.

Принимаем во внимание что запустить автоматизацию может любой, а значит мы отвязываемся от исполнителей и бас-фактора.


## Польза автоматизации для Google SRE

* Гугль видит минусы в использовании сторонних решений и пилит своё.
* Не надо жестко автоматизировать SRE для какого-то прототипа, который возможно
через месяц закроется.

## Применение автоматизации

* Автоматизация - создание программ для решения широкого круга задач. Мета-ПО,
программы, работающие с программами.

Цель создания автоматизации и задачи ей решаемые - могут отличаться.

### Примеры задач для автоматизации:

* Создание пользовательских учеток
* Влючение / отключение кластеров сервисов.
* Подготовка к установке или выводу из эксплуатации ПО или хардваря.
* Развертывание новых версий ПО
* Изменение конфигов ПО
* Изменения согласно зависимостям (особый случай изменения конфигов).
* и т.д. до бесконечности.

### Применение автоматизации в Google SRE

В Гугле есть все перечисленные выше автоматизации.
SRE в первую очередь стремится поддерживать инфраструктуру,
а не качество проходящих данных.
Например, ставим обновление, пропала половина данных.
Гугль пилят оповещения обо всех масштабных изменениях данных.
Но вряд ли SRE будут писать программу, меняющую содержимое учеток системы.
*Dzenly: Непонятно что хотели сказать. Имеются в виду учетки БД ? Т.е. написание миграций за разработчиков ?*
Автоматизация для SRE важна как средство управления жизненными циклами систем,
например, развертыванием сервиса на новом кластеер, а не как средство управления
их данными.

Puppet, Chef, cfengine, Perl, доступны многим. Какие-то низкоуровневые,
какие-то из коробки дают многое, но имеют свои недостатки.
Например, мы часто считаем, что отправка нового файла программы - атомарная операция:
т.е. в итоге в кластере либо останется старая или появится новая версия.
Однако в реальности все сложнее и система может попасть в нестабильное состояние
(сбои в сетях, в хардваре).
Соответственно новые бинарники могут попасть в stage, но не попасть в прод (staged but not pushed. Dzenly: или они имеют в виду закоммичены но не запушены? Да не, врядли. Stage это ещё даже не коммит.),
или попасть в прод но без рестарта, или перезапущены, но не доступные для проверки.
Немного абстракций могут это все разруливать, они либо прекращают работу
и требуют вмешательства инженера, либо вообще ничего не делают (делают вид что всё ок?).

В гугле все сложно, и как правило применяются инструменты без сильных абстракций.

### Уровни автоматизации

В идеальном мире лучше иметь систему, не нуждающуюся в стыковочной логике.
Внутренняя реализация более эффективна.
Нужно выделить ситуации использования стыковочной логики, как правило это
первоочередные действия, типа добавления учеток или запуска системы,
и поискать способы сделать это из самой системы.

Внешним автоматизациям приходится поддерживать консистентность с системой,
когда она меняется. Это все требует тщательного планирования работ и расстановки
приоритетов. Также надо тестировать развертывание после каждого изменения.
Критически важная, но редко исполняемая (а значит и тестируемая) часть автоматизации
зачастую оказывается особенно уязвимой.
Например - восстановление после отказа кластера - редко выполняемая автоматизация (может раз в несколько месяцев).
И если спецом не тестить, - система и автоматизаци могут разъехаться.

#### Путь развития автоматизации

* Нет автоматизации. Восстановление БД после отказа вручную на всех площадках.
* Управляемая извне автоматизация, характерная для данного типа систем.
SRE создал сценарий для восстановления в своей домашней директории.
* Управляемая извне общая автоматизация. SRE добавил поддержку восстановления
в общий сценарий, который юзают все.
* Управляемая изнутри автоматизация, специфичная для данного типа систем.
БД поставляется с собственным сценарием восстановления.
* Системы, для которых автоматизация не требуется. БД сама обнаруживает проблемы,
и автоматом восстанавливается.

Есть ещё автоматизация, которая затрагивает несколько систем.
Например, изменения в цепочке серверов Chubby, или флагов в клиентской либе Bigtable,
для повышения надежности доступа.
Вручную вносить их запаришься. Даже если это просто "перезапустить и проверить".

## Автоматизируе все и исключаем себя из процесса

Юзали MySQL, автоматизировали почти всю рутину по реплицированию.
И тут захотели мигрировать MySQL на Borg. Чтобы Borg обрабатывал запуск/перезапуск аварийно завершившихся задач. И чтобы была возможность запускать в контейнере несколько экземпляров MySQL.
Но встретили сложность, Borg двигает свои таски пару раз в неделю.
Для реплик это норм, а вот для мастеров - нет.

Файловер мастера длился 30-90 минут на инстанс.
Потому что мы работали на общих машинах, и вынуждены были ждать регулярных апдейтов ядра, в добавок к обычной частоте отказов машин нам пришлось учитывать
какое-то кол-во несвязанных отказов каждую неделю.
Это в сочетании с шардированием означало что:

* Ручные файловеры съедали существенные ресурсы человеков и не давали поднять availability более 99%, что было мало для бизнес требований к продукту.
* Чтобы выполнить наш error budget, каждый файловер должен был занимать не более 30 сек даунтайма. Если это зависит от человека, то в 30 сек уложиться невозможно.

Они написали свой автоматический демон для файловеров - Decider.
Он укладывался в 30 сек в 95% случаев и MySQL on Borg стал реальностью.
Для этого пришлось добавить логику обработки отказов в приложение,
т.е. кастомизировать JDBC. Но в итоге нагрузка на человеков уменьшилась на 95%.
Освободилось время для другой работы, и они наавтоматизировали ещё много чего,
и освободили 60% машин.

### Автоматизируем процесс запуска кластера

Жила-была команда. И с каждым запуском нового кластера в неё нанимали
доп. человека.
Для подготовки кластера выполнялись след. действия:
* Оборудование дата-центра питанием и охлаждением.
* Установить основные коммутаторы и настроить соединения с объединяющей магистралью.
* Установить несколько первых стоек с серверами.
* Сконфигурять базовые сервисы, типа DNS и установщиков, затем сконфигурять
сервисы блокировки, хранения, вычисления (это оказалось сложно).
* Развернуть остальные стойки серверов.
* Распределить ресурсы сервисов, с которыми работают пользователи,
чтобы команды инженеров настроили эти сервисы (и это оказалось сложно).

Сложно было потому, что подсистемы хранения и вычисления были в стадии разработки
и часто менялись.
Сервисы имели больше сотни подсистем-компонентов,
и каждая имела сложную сеть зависимостей. Неверная конфигурация одного из компонентов,
могла привести к сбоям у пользователей.

Многопетабайтный кластер Bigtable по соображениям быстродействия
был сконфигурен, чтобы не использовать первый (журналирующий) диск из 12.
Год спустя программа автоматизации решила, что если первы диск не юзается,
то и остальные тоже и уничтожила все данные. Благо были бакапы.

### Выявление несоответствий сп помощью Prodtest

Кол-во кластеров растет, кол-во ручных настроек растет.
И все больше было конфликтов между флагами.
Баш скрипты особо не масштабировались ни от кол-ва людей, желающих внести изменения,
ни в зависимости от общего кол-ва изменений, нужных для кластера.
Также скрипты не проверяли:

* Доступны ли все компоненты, от которых зависит сервис, и корректно ли они сконфигурированы?
* Согласуются ли все конфиги и пакеты с другими конфигами и пакетами?
* Может ли команда подтвердить, что все исключения, сделанные при конфигурации,
осознанны?

Они написали на Python движок Prodtest, для юнит(Dzenly: скорее интеграционного),
тестирования цепочек зависимостей. С визуализацией графов выполнения тестов.
Prodtest расширялся на основе кейсов с ошибками.

Менеджеры смогли прогнозировать время на пуск кластера, и начали понимать
на что тратятся полтора месяца для перехода кластера из состояния "готов к работе"
в состояние "реально работаю с реальным трафиком".
Но SRE получили задание - через три месяца 5 кластеров будут "готовы к работе",
запустите их в течение недели.

### Идемпонентное разрешение несоответствий

Чтобы решить задачу запуска за неделю, они перешли от парадигмы:
* Юнит тесты на Python исплользуются для поиска несоответствия конфигураций
к парадигме
* Python код применяется для исправления несоответствия конфигурации.

Т.е. они объединили тесты с исправляющим кодом.

### Движение к специализации

#### Характеристики автоматизации

* *Компетентность* - способность отдавать корректные, точные результаты.
* *Задержка* - быстрота выполнение шагов инициализации.
* *Релевантность* - доля процессов с автоматизацией.

Запускаетя и обслуживается владельцами сервиса - высокая Компентентность.
Высокая задержка - владельцы сервисов выполняли процессы в свободное время или поручали их новым инженерам.
Высокая релевантность - Владельцы сервисов знали когда меняется ситуация в
мире и могли исправить решение по автоматизации.

Но все меняется, люди потеряли экспертизу. Тесты стали неактуальными.

* Команда, по ускорению текущего процесса ввода кластера в строй,
не заинтересована снижать техдолг владельцев сервиса, которым выпускать его в промышленную эксплуатацию.

* Если сам не юзаешь автоматизацию, то и нет стимула пилить систему, которую легко автоматизировать.

* Если плохая автоматизация не влияет на расписания продакт менеджера,
он будет отдавать предпочтение новому функционалу.

**Наиболее функциональные инструменты обычно создаются людьми, работающиими с ними.**
Поэтому и разработчикам важно наблюдать за своими системами в промышленной эксплуатации.

Процесс ввода кластеров в строй стал некомпетентным, долгим, неточным.
Dzenly: ? Дальше абзац про то как они настроили права доступа, и это каким-то
боком их спасло. Возможно аудит их спас ?

### Запуск кластера, ориентированный на сервисы

SRE перешли от написания shell скриптов в домашних каталогов к построению взаимо
просматриваемых RPC серверов с подробными ACL.

SOA (Service Oriented Architecture).  Владельцы сервиса должны
отвечать за создание экземпляров демона Admin Server, для обработки RPC
по запуску и останову кластера, к которым обратится система,когда
кластеры перейдут в состояние готовности.

Каждая команда (разработки?) предоставляла контракт API, небходимый для автоматизации
ввода кластера в строй. И под капотом могла менять реализацию как хочет.

## Borg: появление компьютера размером с дом

Стойки с оборудованием.
Вход на одном из хорошо известных мастер-компов для выполнения админских заданий.
Эталонные копии бинарнников и конфигов лежали на этих машинах.

В общем история про то, как они построили кластер машин, с самовосстанавливающимися машинами.

## Основное качество - надежность.

Автоматизация работает, люди перестают в ней шарить. Автоматизация дает сбой
и некому чинить.
Автономность систем - все-равно хорошо.

### Сбои при масштабирования

Автоматизация замен стоек и очистки дисков.
Однажды был сбой после очистки.
DiskErase увидел что список пустой, и интерпретировал это как "все машины",
и запустил удаление всех машин с CDN.
Два дня доводили до ума CND, и пару недель пилили идемпотентность в процесс списания машин.

===========================

# 8. Технология выпуска ПО

TODO: до части III



===========================

# III. Практики

Пирамида Маслоу для работоспособности системы.

# Мониторинг

## Реагирование в критических ситуациях

Дежурства это ещё и способ узнать насколько хорошо все работает.

Иерархия:

* Наблюдение
* Реагирование на инциденты
* Постмортем анализ
* Тестирование + процедуры новой версии
* Планирование мощностей
* Разработка
* Продукт

Не обязательно прям сразу исправлять проблему раз и навсегда.
Сначала имеет смысл остановить кровотечение.
Например, временно отключив какой-то функционал, или перенаправив
трафик на другой экземпляр сервиса.

## Постмортем и анализ root-cause

Гугль стремится настроить систему так, чтобы получать оповещения
**только по поводу новых** и действительно существенных проблем.

## Тестирование

Как только мы поняли, что что-то начинает идти не по плану, - следующий шаг -
предотвратить инцидент. Предотвратить болезнь лучше чем вылечить когда она появилась.
Тесты дают какую-то уверенность, что каких-то классов ошибок у нашего софта нет.

## Планирование мощностей

У Гугля есть Auxon, - тулза для автоматического планирования мощностей.
Лоад балансинг обеспечит правильное использование наших мощностей.

## Разработка

Проектирование крупномасштабных систем и инженерная работа внутри организации.
Целостность данных.

## Продукт

Наверху пирамиды надежности - рабочий продукт.

## Другие источники от Google SRE

Процесс тестирования хрупок, и ошибки в нем - могут сильно повлиять на общую стабильность.

Тут у них ссылки на:
* тестирование устойчивости,
* планирование производительности,
* безопасность

==============

# 10 Оповещения на основе данных временных рядов

Благодаря мониторингу:
* Владельцы могут понимать влияние изменений на сервис,
* Грамотно реагировать на критические ситуации,
* Обосновывать необходимость самого сервиса, оценивать насколько он соответствует
бизнес-целям.

Мониторинг крупной системы сложен:
* Огромное кол-во компонентов
* Ограниченные ресурсы инженеров, + их можно грузить не на 100%.

Важно замерять распределение допустим, задержки среди всех серверов в данном регионе.
Тогда можно понять какие факторы влияют на задержку.

В крупномасштабных системах неприемлимо посылать оповещения обо всех сбоях на всех машинах.
Нужно создавать приложения устойчивые к сбоям.
Крупная система должна собирать сигналы и отсекать ненужное.
Т.е. система мониторинга должна алертить о самых важных показателях сервиса,
но при этом иметь детали, чтобы если что поисследовать отдельные компоненты.

В старой системе снимались показания (через выполнение пользовательских сценариев) и рассылались алерты.
В новой - собираются временные ряды, и есть мощный язык для преобразования временных
рядов в графики и оповещения.
Я так понимаю в этом BorgMon из коробки много чего идет.

## Укрепление позиций Borgmon

Prometheus рулит, и похож на Borgmon.
Не надо по всяким ssh куда-то лазить, все из одного места.
Мониторинг методом белого ящика.

Данные копятся, анализируются. Шлются оповещения.

```
curl http://webserver:80/varz
http_requests 37
errors_total 12
```

Крупные сервисы разбиваются ниже уровня кластера на множество
экземпляров-скрапперов. Они наполняют данными экземпляр,
работающий на уровне кластера.

## Инструментарий для приложений

```
http_responses map:code 200:25 404:0 500:12
```

## Сбор экспортированных данных

Для поиска объектов экземпляр Borgmon настраивается
согласно списку целей.
С использованием одного из методов разрешения имен.
Раз в какой-то интервал опрашивает разные varz с сервисов,
и расширяет коллекцию для каждого экземпляра на весь интервал.

Также пишутся мета данные:

* Отрезолвилось ли имя в хост и порт.
* Ответил ли объект на запрос о сборе данных.
* ответил ли объект на проверку работоспособности.
* в какое время завершился сбор данных.

Всех этих данных хватило для написания правил
для проверки доступности наблюдаемых задач.

SNMP разработан, чтобы соответствовать мин. требованиям для передачи данных и продолжать работать, когда другие сетевые приложения
подыхают.

HTTP для скраппинга противоречит этому принципу,
но это редко становится проблемой.
Dzenly: ?Т.к. часто симптомов достаточно, и не обязательно копать сильно вглубь?

Система разработана устойчивой к сбоям, и эти сбои можно юзать в правилах.

## Память временных рядов как хранилище данных

Borgman хранит много чего в оперативке и время от времени сбрасывает на диск.

12 часов - временной горизонт (интервал между самой
свежей и самой старой записями).

Метки.
Как минимум:

* var, job (задача), service (коллекция задач), zone (местоположение, обычно дата-центр).

...

Можно добавлять разные метки.
Ну и дальше рассказывается про разные плюшки, типа что можно запросить
за интервал времени. Вроде это все есть в прометее.

## Вычисление правил

Централизованно, в Borgmon.
Конфиги в одном месте.
Можно запускать правила параллельно, можно последовательно.
Например, если медленные запросы - можно следующим последовательным
правилом - дернуть метрики по вычислительным ресурсам.

Агрегирование. Общая интенсивность запросов к job - сумма
интенсивности изменений всех тасок.

Total QPS rate джобы в датацентре это сумма
всех скоростей изменения всех каунтеров.

счетчики - только увеличиваются (пройденные километры).
индикаторы - могут иметь любое значение (кол-во топлива, или текущая скорость).
Счетчики сохраняют историю, индикаторы нет (инфа между запросами данных теряется)

Например, для веб сервера хотим добавить алерт, при превышении
процента ошибок над SLI.
Т.е. когда сумма частоты не 200 кодов по всем таскам в кластере,
деленная на сумму частот запросов всех тасок в кластере
больше чем сколько-то.

### Это можно сделать так: (стр 164)






















