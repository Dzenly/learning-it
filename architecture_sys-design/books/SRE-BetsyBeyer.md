Site Reliability Engineering, edited by Betsy Beyer,
Chris Jones, Jennifer Petoff, and Niall Richard Murphy (O’Reilly). Copyright 2016
Google, Inc., 978-1-491-92912-4.

https://www.youtube.com/watch?v=q68S0wKy5vE

После появления ПО продукта много сил (40-90%) надо на поддержку и развитие.

# Цикл жизни ПО

* Создание
* Развертывание
* Функционирование
* Обновление
* Вывод из эксплуатации

====

Для разного ПО есть своё понятие "достаточно надежности".
АЭС - одно, интернет - магазин - другое.

====

Site в SRE - легаси.

====

SRE отличается от DevOps тем, что в нем упор идет на надежность.

====

Даже доскональная документация и жирные красные шрифты чего делать
не надо, не гарантируют отсутствие ошибок.
Люди все-равно рано или поздно сделают что-то запрещенное документацией.

====

Учет рисков - ключевой момент в SRE.

====

Надеяться - это плохая стратегия.

Operations (ops) - служба эксплуатации (развертывание, дежурство).

Без автоматизации OPS масштабируется только за счет найма админов.

Цели Ops и Dev противоречат друг другу.

Ops проверяют продукт.
Dev дробят продукт, чтобы проверять
нужно было только измененную часть.

Гугль в SRE команды набирает разработчиков, чтобы они
там по-максимуму все автоматизировали.

Это либо чистые разработчики без навыков админства (40-50%),
либо разработчики с навыками админства (UNIX, Network (OSI L1-L3)). Но всегда разработчики.

SRE разрабатывают приложения для решения сложных задач,
т.е. автоматизируют Ops.
Постоянно дорабатывают и улучшают свою автоматизацию.

**Гуглевский лимит - не больше 50% на операционку.**
Как раз чтобы было время на улучшение автоматизации.
В идеале стремятся к 100% автоматизации.

Как достигать этих < 50% операционки: перекладывать часть на Dev (весь избыток над 50%, вплоть до привлечения разработчиков к дежурствам и переадресации поступающих ошибок менеджерам разработки) или введение в SRE команду людей, на которых нет операционки.

При таком грамотном подходе, при масштабировании системы
SRE команда растет медленнее, чем система.

Ротации между разработчиками и SRE/

Из трудностей: т.к. в обе команды идут программисты -
конкуренция за людей с рынка труда.

**В менеджменте есть свои трудности**: например, остановить
выпуск версий до конца квартала, т.к. исчерпан лимит
времени недоступности сервиса.**

DevOps - обобщение SRE для широкого круга организаций, управленческих структур и персанала.
SRE - имплементация DevOps с расширениями.

# SRE команда отвечает за:

* Доступность
* Время отклика
* Производительность
* ?Эффективность?
* Управление изменениями
* Мониторинг
* Реагирование в аварийных и критических ситуациях
* Планирование производительности

=======

В организации должны быть систематизированы правила и принципы взаимодействия команд SRE с:
* Сопровождаемыми системами
* С командами разработчиков
* С командами тестировщиков
* С пользователями
* И т.д.

========

Когда разработчики участвуют в SRE (во время авралов, например),
они получают непосредственную обратную связь,
чтобы пилить системы, не требующие большой поддержки.

Если появляется больше двух проблем за 8-12 часовую смену,
- уже не хватает времени хорошо изучить проблему
и поправить архитектуру.

Но если в среднем меньше 1 события за дежурство - работа на месте дежурного - пустая трата времени.
? Почему, ведь можно в перерывах пилить свои задачи,
или добивать техдолг по архитектуре?

Нужно писать отчеты с анализом причин произошедшего (постмортемы)
для всех значимых инцидентов.
Особенно для событий, прошедших мимо мониторинга и алертинга.
Расследование должно установить все детали, и выработать план действий по устранению
или более эффективной обработки подобных инцидентов в будущем.
Чтобы ошибки не замалчивались - важно максимально избегать обвинений.

**Суммарный уровень/бюджет ошибок** (error budget)
Избавление от конфликта Dev vs Ops.

Наблюдение: 100% надежность необоснована в большинстве ситуаций (кроме, допустим: АЭС, кардиостимуляторов, систем безопасности автомобилей).

Разница в 99.999% и 100% теряется на фоне случайных факторов, и пользователь
не получает выхлопа от наших потраченных сил для добавления ещё одной 9ки.

# Вопросы к продукту при проектировании:

* Какой показатель доступности удовлетворит пользователей.
* Какие альтернативы имеют пользователи, неудовлетворенные доступностью продукта.
* Как изменится использование юзером продукта при разных уровнях доступности.

======

`ErrorBudget = 1 - PlannedAvailabilityTarget`

Этот бюджет можно тратить например, на
внедрение нового функционала и привлечение новых пользователей.

Можно освобождать error budget с помощью:
* Поэтапного развертывания (phased rollout).
* И экспериментального 1 процента (канареечные деплои ?)

В итоге Ops и Dev договариваются совместно на что потретить этот
error budget.
Т.е. баги и сбои не считаются чем-то плохим, ожидаемы, и входят
в тактические планы.
Исчезает страх, появляется лучший контроль.

======

# Мониторинг

* Почта неэффективна, если недостаточно очевидно что надо делать,
или если бывает что это просто некая инфа и делать с ней ничего не  надо.

* Срочные алерты (*alerts*) - нужно быстро отреагировать, т.к.
либо вот-вот произойдет, либо уже произошло что-то плохое.
* Запросы на действия (*tickets*) - Нужно вмешаться, но не обязательно сейчас, можно в течение нескольких дней.
* Журналирование (*logging*) - Пишется на всякий случай, для возможного последующего анализа для тикетов и алертов.

# Реакция на критические ситуации

MTTF - Mean Time To Faifure - Среднее время безотказной работы
MTTR - Mean Time To Repair - Среднее время восстановления

Ручные вмешательства - долго.
Продумывание всех деталей и **прописывание рекомендаций в инструкцию** - утраивает скорость восстановления системы, по сравнению с импровизациями.

Игры "Колесо неудачи" (Wheel of Misfortune)

============

# Управление изменениями

70% сбоев происходит из-за изменений в работающей системе.

## Нужно юзать автоматизацию для
* Поэтапного развертывания обновлений ПО;
* Быстрого и точного выявления проблем;
* Безопасного отката изменений при ошибках.

Типа убираем человеческий фактор: всякие усталости, невнимательности, замыленности, и т.д.

# Прогнозирование нагрузки и планирование производительности (ресурсов, мощностей)

* Закладывать небольшую избыточность?
* Нужно **как можно точнее** учитывать и естественный постепенный рост, и скачкообразные росты (новый функционал, сезонные эффекты, и т.д.). В том числе и за пределами срока, требуемого для ввода новых мощностей.
* Регулярное НТ, с отслеживанием насколько в каких местах грузится железо.

Команда SRE должна отвечать за все это планирование и за железо
(материально-техническое обеспечение (provisioning)).

# Мат-Тех обеспечение (Provisioning)

Оборудование дорогое. Надо делать все быстро и правильно, но только когда это
правда необходимо.

Наращивание производительности это: введение новых инстансов систем,
площадок размещения инфры, внесение изменений в конфиги, балансеры,
сетевые настройки, и т.д. Плюс проверка что все работает ожидаемо.

# Эфективное использование ресурсов

Коэффициент использования ресурсов влияет на стоимость сервиса.
При каком-то критичном перегрузе - может быть полный останов системы.

==============

# Оборудование

* Машина (комп, возможно виртуальная)
* Сервер - еденица ПО, реализующая сервис.

Сервера не привязаны к машинам. Ресурсы распределяются системой *Borg*.

* *Стойки* (десятки машин)
* *Ряды* стоек.
* Несколько рядов - *кластер*.
* Несколько кластеров - *ЦОД*.
* *Кампус* - Несколько близко расположенных ЦОДов.

Крутая сеть. Jupiter.
B4, OpenFlow.

==============

Для себя: видимо SRE все-таки должно норм владеть аппаратной частью,
чтобы понимать какие нагрузки какое оборудование держит.
Или даже писать софт, которое управляет конфигами оборудования.
Ну и надо понимать, что железо ломается, и не является
чем-то надежным.

==============

# ПО для оборудования

В крупных кластерах сбои оборудования случаются часто.
За год могут сломаться тысячи машин и жестких дисков.
**В общем, железо - расходник, не стоит рассчитывать на его надежность**

* В каждом кампусе есть команда, отвечающая за поддержку оборудования,
и инфраструктуру дата-центра.

## Управление машинами

*Borg* (он породил Кубер, похож на Apache Mesos)
borglet - kublet.

Borg запускает jobs. Это могут быть как постоянные сервисы,
так и процессы пакетной обработки вроде MapReduce.
Могут быть тысячи задач.
Работает примерно как кубер.

IP не статические.
Borg Naming Service, выделяется индекс для задачи.
`/bns/<cluster>/<user>/<job_name>/<job_index>`
И это уже можно преобразовать в `IP:Port`.

## Хранилище

Некая распределенная файловая система, похожая на HDFS или Lustre.

Disk(диски, флеши)->Colossus(FS, replication, cyphering, наследник Google File System)->BigTable (NoSQL, распределенная, разреженная, отказоустойчивая, многомерная БД, индексируется по ключам строк, столбцов и временным метком, каждое значение - произвольный массив байт, поддерживает репликацию)->Spanner (SQL для пользователей, где нужна целостность и согласованность).

Есть ещё всякие blobstore.

## Сеть

OpenFlow
Вместо умных и дорогих роутеров - глупые и дешевые коммутаторы + центральный контроллер, вычисляющий лучшие маршруты.

Bandwidth Enforcer (BwE) управляет доступеной полосой. Максимизирует
среднюю пропускную способность.
Централизованное управление трафиком решает проблемы,
которые плохо решаются распределенной маршрутизацией.

### Global Software Load Balancer, GSLB - три уровня:

* Географическая балансировка нагрузки для DNS запросов.
* Балансировка на уровне пользовательских интерфейсов (YouTobe, GMaps);
* Балансировка на уровне RPC.

Как данные для балансировки используются:
* Символьные имена, список BNS адресов.
* Производительность, доступная на каждой площадке (QPS, queries per second).

# Другое системное ПО

## Сервис блокировок (Chubby)
Предоставляет API, схожий с файловой системой.
Юзает Paxos для обращения к Consensus.
Играет роль при выборе мастера из реплик.
BNS использует Chubby

## Мониторинг и алертинг

Borgmon

* Оповещения о неотложных проблемах
* Информация для оценки как очередное ПО влияет на производительность
* Информация как потребляются ресурсы (допустим, с ростом кол-ва клиентов, или в какие-то сезонные моменты.). Все это - инфа для планирования нагрузки.

# Инфраструктура ПО

Сервисы умеют в многопоточность, и предоставляют HTTP интерфейс,
для сборка диагностической инфы и статистики.
Сервисы общаются по gRPC.

Часто локально вызывают RPCшные функции.
Потом легко отделить сервис.
GSLB - балансирует gRPC нагрузку.

Сервер получает RPC с фронта и отправляет их в бакэнд.
protobuf.

# Среда разработки

Общий репозиторий??
Можно делать МР в чьих-то файлах.
Ревью МР.
Параллельная компиляция.
CL - change list.
Тестирование кусков, на которые эти изменения **прямо или косвенно** могут повлиять.
**push-on-green** - Отправка в прод при успехе.

# Shakespeare: пример сервиса

* Читаем все найденные пока что тексты Шекспира и запихиваем их в Bigtable. Если найдем новые тексты - вызовем процедуру ещё раз.
* Фронт - для связи с пользователями.

## map-reduce
* Mapping: все тексты разбиваются на отдельные слова. Процесс идет параллельно.
* Shuffle - сортировка по словам.
* Reduce - индексы, кортежи вида (слово, список произведений).
Каждый кортеж - строка в Bigtable, ключ - слово.

## Жизненный цикл запроса

GSLB по символьному имени определяет какой IP возвратить пользователю.
Браузер соединяется с Reverse Proxy (Google Front End), он ищет сервис,в данном случае Shakespeare.
Дальше идет RPC на фронтэнд-сервер Shakespeare. И дальше запрос упаковывается в protobuf и идет на бакэнд-сервер. Через BNS, чтобы тоже балансировать нагрузку. Бакэнд запрашивает данные в Bigtable.
Опять protobuf, фронт-сервер, который создает HTML для ответа юзеру.

Все работает быстро и надежно.
Упреждающие методы восстановления при ошибках - (*постепенное отключение функций*.)

## Организация задач и данных

Допустим бэк может обработать 100 QPS.
Допустим, поняли, что пиковая нагрузка будет 3740 QPS.
Значит надо около 35 бэков.

Но во время обновления одна из задач всегда недоступна, и ещё можно учесть вероятность сбоев.
Поэтому добавим + 2 бэка.

Дальше можно поанализировать нагрузку по регионам, и раскидать
приложение по датацентрам этих регионов соответственно.
N+2 для каждого региона.
Для устойчивости - можно учесть межрегиональные перебросы.
Для дешевизны пренебречь N+2, оставив N+1 для малонагруженных регионов.

Bigtable дублируется в каждом регионе.
Консистентность прямо сию секунду не нужна.

=============

# Принципы

* Оценка рисков
* Управление рисками
* Использование лимита недоступности (error budget)

**SLO** Service Level Objectives

=============

# Приручаем риски

* Надежность не бесплатна. Каждая новая девятка может стоить в 100 раз больше предыдущей.
* Систему не имеет смысл делать сильно надежней, чем компоненты, через которые юзер работает с системой.

Цены:

* Машины. Резервные для переключения при сбоях или при обслуживании. А также хранящие контрольные суммы для обеспечения целостности.
* Упущенные возможности, когда разработчиков перебрасывают в SRE.

Строим графици стоимость, прибыль, уровень надежности, риски.

*Незапланированные отключения* как объективная метрика надежности.

`Availability = Uptime / (Uptime + Downtime).`

99.99 - 53 минуты простоя в год.

Но вообще **кол-во успешных запросов** за какой-то отрезок времени как метрика показательней.

`Availability = Successful requests / total requests.`

Допустим, Потеря 250 запросов из 2.5 млн запросов в день - 99.99%.
Есть разные типы запросов, они сильно важные, другие - нет. Так что это не 100% объективная метрика, но она достаточно хороша.

Запросы могут быть и всякие внутренние. Мож даже и не связанные с юзерскими запросами (а мож и связанные).

Доступность гугль замеряет раз в квартал. А производительность
еженедельно или даже ежедневно.

## Рискоустойчивость сервисов

Иногда это критически важно. И тогда понятие рисков включено в характеристики продукта.

Риски оценить не всегда просто.
Инфраструктурные сервисы (системы хранения или уровень кэширования для HTTP) могут не иметь определенного владельца.

### Пользовательские сервисы

Часто имеют команды и продакт овнера (менеджера).
Если команды нет, требования к доступности выполняют
люди, создающие систему, даже если они об этом и не знают.

#### Факторы при оценке рисков

* Какого уровня доступности надо достичь.
* Как различные типы сбоев влияют на сервис.
* Как мы можем менять стоимость сервиса, чтобы позиционировать его на кривой зависимости рисков.
* Какие другие показатели сервиса нужно иметь в виду.

#### Целевой уровень доступности

* Какого уровня доступности будут ожидать клиенты.
* Связан ли сервис непосредственно с прибылью (нашей или пользователей).
* Сервис платный или бесплатный?
* *Какие есть конкуренты, какой уровень сервиса у них.*
* *Сервис для предприятий или для конечных пользователей?*

Google Apps for Work - 99.9%.
+ Более высокий уровень внутренней доступности.
+ Контракт со штрафами, если не выполнили уровень.

А вот когда Google купил YouTube, он поставил уровень надежности поменьше, **чтобы развиваться быстрее**.

#### Типы сбоев

* Что лучше несколько мелких сбоев или один большой?
* Есть полные, есть частичные сбои. Например, в профайле все работает, кроме фоток.
* **Есть сбои, когда лучше полностью отключить сервис**. Например, когда компрометируются данные пользователей.
* Некоторые сервисы используются в определенное время дня, и их можно отключать в другое время.

#### Стоимость

* Что нам стоит добавить ещё 9-ку?
* Сколько мы получим прибыли из-за этого?

Если нет однозначной зависимости прибыли от надежности.
Можно взять за основу надежность сервисов, через которые используют твой сервис.
Например, надежность интернет провайдера пользователя.
Разные инет провайдеры это 99 - 99.99%.

#### Другие показатели сервисов

* Задержка (иногда она даже желательна, например, для показа рекламы)


## Определение рискоустойчивости инфраструктурных сервисов

Отличаются от требований к потребительским продуктам.
Т.к. имеют несколько клиентов, чьи потребности часто не совпадают.

Где-то важна *надежность*, где-то *пропускная способность*,
где-то *латентность*.

* Какое состояние очереди запросов желательно для пользователей каждого типа?

### Типы отказов

* Те, которым нужна низкая латентность - хотя видеть очереди пустыми.
* Те, кому нужна пропускная способность - хотят, чтобы очереди были не пустыми.

### Стоимость

Можно разделить инфру и ввести несколько независимых уровней обслуживания.

Короткие очереди - за счет доп. ресурсов.
Конкуренция между потребителями уменьшается, очереди разгружаются.

Но в 2-10 раз дешевле настроить кластер на максимальную пропускную способность.

Можно раскидать критичные данные и не очень по хранилищам с разными характеристиками и разной стоимостью содержания.

На одном и том же железе и ПО можно настраивать разные классы систем,
за счет конфигов.

### Пример: фронтенд-архитектура

Фронтэнды (реверс прокси, балансировщики) делают максимально надежными. Т.к. тут есть риск полностью потеряь запрос.


## Обоснования критерия суммарного уровня ошибок (errors budget)

Природа конфликта Dev vs Ops.
Разработчиков оценивают по тому, сколько фич они запилили.
Ops оценивают по надежности систем.

* *Устойчивость* к сбоям. Слабая устойчивость а нежданчикам -
будет дырявое в плане безопасности и нестабильное приложение.
Если устойчивость будет сильной, приложение никто не захочет
использовать. *Почему??* Дорого?? Долго, конкуренты опередят?
* *Тестирование*. Мало тестишь - ошибки, разного уровня серьезности.
Долго тестишь - теряешь рынок.
* *Частота релизов*. Каждое обновление у клиента - риск.
* *Продолжительность тестирования и размер выборки*.
Канареечные версии. Как долго ждать и как много юзеров переводить на канарейку.

Во многих компаниях и продуктах это все выбирается необъективно.
Какие-то исторически сложившиеся компромисы.
Лучше стараться оценивать объективно на разных количественных метриках.

## Формируем error budget

Команды его совместно определяют на квартал.
Основываясь на SLO.

Гугль делает так:

* Менеджер продукта определяет SLO. Задавая время uptime в течение квартала.
* Система мониторинга (нейтральная третья сторона) замеряет текущий uptime.
* Разница и есть запас (бюджет) на оставшуюся часть квартала.
* Если есть запас - можно продолжать выпуск обновлений.

Например, если какая-то проблема съедает 0.0002 аптайма, а целевой 0.001. То бюджет ошибок эта проблема выгребает на 20%.

## Преимущества

Поиск баланса между инновациями и надежностью. Вместе. И Ops и Dev.
Если SLO не достигается - приостанавливаем выпуски, дорабатываем продукт. Можно даже откатить релиз.

И тут разработчики подумают стоит ли выпускать релиз, который
может съесть весь бюджет и остановить разработку.
Это если у SRE команды есть право останавливать релизы.

Если обе команды понимают что дело в железе - они сообща
решают приостановить релизы.

SLO не обязан быть константой, при каких-то условиях его можно подправлять.

=================

## Целевой уровень качества обслуживания

### SLI - service level indicators (показатели)
Четко определенное числовое значение конкретной характеристики
обслуживания.
* Время отклика
* Уровень и частота ошибок (процент успешных запросов от всех, даже сформированных некорректно)
* Throughput (QPS)
Берется окно измерения, данные усредняются, рисуются процентили.

Не всегда можно напрямую измерить что-то.
Мы часто знаем время обработки запроса на сервере, но вот задержку у клиента померить сложнее.

* *Доступность* (availability) Доля успешных запросов из тех, что сформированы корректно. Также называется *yield* (выработка, отдача, урожай).
* *Durability* - долговечность. Вероятность что данные не потеряются
за какое-то время.

Две девятки - 99. Пять девяток - 99.999.
Google Compute Engine - 3.5 девяток. Т.е. 99.95%

### SLO

SLI <= Target
lower_bound <= SLI <= upper_bound

I.e. для Shakespeare - средняя латентность поиска должна быть меньше 0.1 сек.

Значит надо пилить фронт с низкой латентностью, или улучшать оборудование.

Выбор SLO сложен. Например, QPS определяется субъективными
ощущениями пользователей.

Есть статья Speed Matters, где проведены исследования как я понял по тому, как скорость влияет на удержании юзеров.

И латентность и QPS могут ухудшаться после определенного предела нагрузки.

Юзеры могут считать систему и более надежной чем она есть и менее надежной.

Например, кто-то там в своих сервисах завязался на Chubby, считая его 100% надежным, и был неприятно удивлен во время их обслуживании.
Иногда чтобы приучать юзеров создают **искусственные сбои**.

### SLA

Явный или нет контракт с юзерами, **включающий последствия**
при соответствии или несоответствии SLO.

**Можно создавать и без SRE**

Google Search - сервис без SLA.

=================

## Показатели на практике

Не надо в SLI пихать все, что есть в мониторинге.
Надо выбрать действительно важные параметры.

* Для *пользовательских сервисов* обычно важны *доступность*, *латентность*, *пропускная способность*.
* Для *систем хранения* - *латентность*, *доступность*, *долговечность*.
* Для *систем обработки больших объемов данных* (Big Data Systems) (pipeline processing) -
*thoughput* and *end-to-end latency* (from ingestion to completion).
Некоторые конвееры могут иметь целевые показаели латентности на индивидуальных этапах).
* *correctness* - For all systems. Правильные ли данные возвращены,
был ли корректен выполненный анализ.
Правда он часто - свойство данных а не системы, и не входит в зону ответственности SRE.

### Сбор показателей

Borgmon или Prometheus.
Журналы приложений.
Иногда данные можно собирать на  **стороне клиента**.
Например скорость обработки JS или метрики по загрузке CDN.

### Агрегация

Часто сырые первичные данные агрегируются. Это нужно делать осторожно.

Если у вас есть выброс в обработке запроса, который сгладился
т.к. вы усредняете по минуте, вы проглядите проблему.

**Лучше смотреть распределения** по персентилям.

50-й персентиль - медиана, типичные ситуации.

Чем выше разброс значений, тем больше влияет на пользователя
хвост распределения. Эффект усугубляется при высоких нагрузках
из-за поведения очередей.

**SRE Можно ориентироваться на какой-нибудь 99.9 процентиль.**

О статических заблуждениях: среднее для IT не то же самое как для физики. Среднее арифметическое может сильно отличаться от медианы.
И закон распределения не факто что будет нормальным.
Это всё нужно учитывать в автоматизации. Например если автотулза
рестартит сервис, она должна полагаться не на интуитивные распределения, а на фактические.

### Стандартизация показателей (SLI)

Можно стандартизовать некие общие показатели, чтобы не думать над
ними каждый раз заново.
Тогда останется думать только над специфическими показателями.

* Интервал агрегации: среднее значение по минуте.
* Регион агрегирования: все задачи в кластере.
* Частота измерений: каждые 10 сек.
* Какие запросы включаем в выборку: GET, полученные мониторингом методом черного ящика.
* Способ получения данных: Получены путем наблюдения. Измерения на стороне сервера.
* Задержка при доступе к данным: Время получения последнего байта.

## Целевые показатели на практике

В первую очередь отталкиваться от того, что важно клиентам.
А не от того, что мы можем измерить.
Т.е. лучше ставить цели, а потом думать как замерить показатели для них.

### Определение целей

Нужна инфа как измерять SLO и условия их действительности.
Например:

* 99% (в среднем за минуту) вызовов RPC Get будут завершены
менее чем за 0.1 сек. (По всем бэкам).
* 99% RPC Get будут завершены менее чем за 0.1 сек. Остальные параметры эта строка берет из SLI defaults.

Если важна форма кривых производительности, можно указать несколько
целевых значений:

* 90% RPC Get будут завершены менее чем за 1ms.
* 99% RPC Get - 10ms
* 99.9% RPC Get - 100ms

Если есть юзеры с разной загруженностью (вроде конвейера обработки
массивов данных), для которых важна throughput и интерактивный клиент, для которого важно время отклика, то для каждого вида
нагрузки лучше задать свои SLI

* 95% RPC Set, для которых важна throughtput, < 1 sec.
* 99% RPC Set, для которых важна latency и которые имеют нагрузку
меньше 1Кб, - будут завершены < 10ms.

SLO не должны соблюдаться 100% времени.
Лучше установить уровень допускаемого несоответствия SLO,
и регулярно его трекать (ежедневно, еженедельно),
тогда можно увидеть, что что-то потихоньку начинает идти не так,
и заранее принимать меры.

+ Ежемесячный или ежеквартальный отчет по использованию error budget.

### Выбор целевых показателей

Что надо учитывать:

* Интересы продукта и бизнеса.
* Ограничения кадрового состава.
* Сроками выхода на рынок.
* Доступность аппаратных средств.
* Финансы.

#### Уроки гугла

* Не выбирать цель, основываясь на текущей производительности.
Лучше безотносительно текущих подумать какие они должны быть.
Возможно где-то можно сэкономить.
* Не усложняйте агрегацию. Тут можно потерять важные данные.
* Не гонитесь за абсолютом. Good enough for users.
* Минимизируйте кол-во SLO до тех, которых достаточно
для получения полной информации о характеристиках системы.
Обосновывайте необходимость SLO. Если среди команд нет согласия,
что данный SLO нужен. То может лучше его и не вводить. Да и возможно
SRE тут не нужно.
* Идеал может подождать. Лучше сначала ориентироваться
на реально доступный уровень, чем тужиться потом.

SLO должны стать главным ориентиром при приоритезации задач SRE
и даже разработчиков.
Чем лучше продуманы SLO, - тем эффективней тратится время Dev и SRE,
и тем более доволен клиент.

### SLO как элемент управления

* Измеряйте SLI
* Сравнивайте с SLO, решайте нужно ли что-то предпринять.
* Решайте что конкретнопредпринять.
* Действуйте.

Например, идет рост времени отклика. Можно временно повысить ресурсы,
и разбираться почему отклик растет.

### Опубликованные SLO формируют ожидания пользователей

Соответственно пользователи имеют больше инфы чтобы выбрать или
не выбрать ваш сервис.
Например, если доступность низкая - это норм для хранения не сильно нужных архивов.

#### Тактики

* Запас прочности. Более жесткие внутренние SLO.
Тогда будет больше времени на разбор проблем и больше маневров занести в продукт что-то тяжелое (но м.б. более дешевое).
* Избегать перевыполнения. Пример с Chubby. Не приучайте юзеров
завязываться на крутые SLO. Время от времени имитируйте отключения.
**Или можно искусственно ограничивать QPS до SLO-шных.**
Или проектировать систему так, чтобы она работала с одинаковой скоростью при разных нагрузках. Т.е. допустим довыделяла себе
только сколько надо ресурсов при больших нагрузках, и возвращала
их обратно при снижении нагрузок.

### Соглашения на практике

Когда думаем над SLO с овнерами и юристами, роль SRE - донести реалистичность выполнения разных SLO.
Чем шире круг пользователей, тем труднее изменить или удалить SLA,
которые оказались трудновыполнимыми или необоснованными.

===========================

# Избавляемся от рутины

*Если при выполнении стандартных операций нужен человек, у вас есть ошибка*
Карл Гейссер.

*Служебная нагрузка* - обсуждения, определение целей и приоритетов,
отчеты, бумажная работа для кадровой службы. Найм людей. Ревью кода.
Рефакторинги. Прохождение обучающих курсов.

*Грязная работа* - очистка конфигурации системы оповещения,
удаление мусора. В перспективе имеет ценность.Но это не рутина. От меня: ??Правда?? По мне так это можно автоматизировать, а значит рутина.
Разбирание с устаревшим кодом - грязная работа, но ведет к улучшениям,
не рутина.

*Рутина* - имеющая один или несколько из след. признаков:
* ручная (Ручной запуск автоматизированного сценария - рутина)
* однообразная (т.е. встречающаяся не первый раз),
* поддающаяся автоматизации (если в процессе автоматизированного решения, человек должен что-то решить это не совсем рутина, но возможно можно переделать систему, что участие человек не понадобится)
* операционка по поддержке работающего сервиса (реакция на алерты например).
* Результаты не имеют ценности в перспективе (если сервис после ваших действий не улучшился - скорее всего рутина).
* Трудоемкость растет линейно по мере роста сервиса. Хорошо спроектированный
сервис должен масштабироваться на порядок без нужды в человеке.

## Почему важно минимизировать рутину

Если с рутиной не бороться, она съест.
В гугле на рутину отводится не больше 50% (в среднем за несколько кварталов или год) и жестко выдерживаются эти рамки.
Если нарушаем - разбираемся.

### Определяем кол-во рутины

* SRE одну неделю цикла проводит как дежурный первой очереди
и одну неделю - как дежурный второй очереди.
* Допустим в ротации участвует 6 человек.
Значит 33% на рутину.
Если 8 - 25%.
Я так понимаю верхняя граница 50%.
* Главный источник рутины - поступающие запросы (например, несрочные сообщения, связанные с работой сервиса).
* Следующий источник (я так понимаю не по приоритету, а по объему), -
срочные ответы по запросу с последующими релизами и пушами (что блин?? в общем какие-то дела как-то связанные с релизами и пушами).
* Ежеквартальные опросы показывают что на рутину гугль SRE тратят
примерно 33%, что лучше целевых 50%.
Но это в среднем по больнице. На самом деле есть и 0%, а есть и 80%.
И тут надо менеджерам балансировать.

## Что такое инженерная работа

Проектирование универсальных и общих решений, реализация автоматизации.
Позволяет не увеличивать персонал при масштабировании сервисов.

* *Разработка ПО*. Написание авто-сценариев.
Создание инструментов и фреймворков.
Добавление функционала для масштабирования и надежности,
повышение надежности инфраструктурного кода.
* *Разработка систем* Конфигурирование, документирование, рефакторинг для последующего удобства работы с настройкой.
Настройка мониторинга, обновлений, балансировщиков нагрузки, параметров ОС.
Консультация разработчиков по архитектуре и передаче в эксплуатацию.
* *Рутина* по обслуживанию сервисов.
* *Доп. служебная нагрузка* (см. выше).

## Всегда ли рутина вредна

* Однообразная работа может кому-то быть и норм.
* Полностью избежать рутины невозможно.

### Почему избыток рутины - плохо
* *Застой в карьере*. Нужно участвовать в проработках и реализации автоматизации.
* *Снижение производительности*. Выгорание, скука, недовольство.
* *Если делаешь рутину, то не делаешь инженерные задачи*, соответственно проекты перестают нормально масштабироваться.
* *Путаница*. Если ты транслируешь, что рутина - норм, остальные не понимают зачем мы её автоматизируем тогда.
* *Замедляется прогресс*. Рутина растет. Меньше времени на нерутинные задачи. Выпуск релизов, и т.д.
* *Нежелательные прецедент* Если вам рутина норм - на вас её вешают все больше, снимая с себя и расслабляясь (особенно касается разработчики). Привыкают, считают нормой.
* *Потери личного состава* - Если команду перегрузить рутиной - люди начнут искать места поинтереснее.
* *Подрыв веры* - на собесе сказали, что будет много автоматизации,
в реале - рутина, что негативно влияет на боевой дух и производительность.

======================

# Мониторинг распределенных систем

В индустрии типа есть проблемы с терминами.
* *Мониторинг (наблюдение)* Сбор, обработка, агрегирование и отображение
в реальном времени количественных показателей системы, например общее
число и тип запросов, кол-во и тип ошибок, время обработки запросов, аптайм сервера.
* *Наблюдение методом белого ящика* - Использование показателей,
доступных внутри системы. E.g. Java Virtual Machine Profiling Interface, обработчики HTTP по внутренней статистике.
* *Наблюдение методом черного ящика* - Наблюдение и проверка (тестирования) поведения видимого извне, как это видит юзер.
* *Информационная панель (панель управления) (Dashboard)* Приложение (обычно веб), предоставляющее сводку показателей сервиса. Может иметь фильтры, селекторы
и т.д. Но заранее её конфигурируют так, чтобы показывать наиболее
важные для пользователей сведения.
Может также отображать важные для команды сведения, типа длины очереди тикетов,
списка наиболее приоритетных ошибок, имени дежурного инженера,
инфа о последних установках ПО.
* *Оповещение (алерт)* - Сообщения, на которые должен обратить внимание
человек. Направляются в очередь тикетов, имейл (как правило неэффективно), пейджер, телефон (вызовы, экстренные оповещения).
* *Основная причина* (Root Cause) - Дефект в ПО, или человеческая ошибка, после
исправления которой аналогичное событие не произойдет.
Может быть *несколько основных причин*.
Недостаточная автоматизация,
программный сбой из-за некорректных входных данных,
недостаточно качественное тестирование сценария конфигурации.
И т.д.
* *Узел или машина (Node)* - Один экземпляр ядра (?приложения?), на физическом или виртуальном сервере или в контейнере.
На одной ноде м.б. стоит мониторить несколько сервисов.
Серверы могут быть:
* Либо связаны друг с другом, например, кэш и веб сервер.
* Несвязанные сервисы, шарящие хардварь: например, репу кода, или мастер конфигурационной системы
типа Puppet или Chef.
* *Push* - Любые изменения в софте или конфигурации.

## Зачем мониторить

* *Анализ и экстраполяция тенденций* Как быстро растет БД, QPS, кол-во пользователей.
* *Сравнение с предыдущими версиями или экспериментальными группами (канарейками?)* Как работают новые БД или версии старых БД. Улучшаются ли характеристики с добавлением доп. узлов. Сравнение работы сервиса
с прошлой неделей.
* *Оповещение* Что-то либо уже сломалось. Либо вот-вот сломается. Кому-то
нужно заняться.
* *Создание информационных панелей* (**4 golden signals** см. дальше).
* *Ретро анализ (например, отладка)* Допустим мы поняли что возрасло врема отклика, что ещё случилось в это время.
* *Бизнес-аналитика*
* *Анализ брешей в защите*

Сообщения должны быть по делу.
Тикеты "что-то немного странно выглядит" - не должны отвлекать инженеров. Если речь не о мега-критичных вещах.

Вызов инженера - очень дорог.
Если вызовы часто - работает "волки волки".
Не должно быть шумов, чтобы сигнал/шум, было большим.

## Реальные задачи для мониторинга

* Мониторинг за крупным приложением - сложная задача.
Команда может выделять до 20% ресурсов на создание и обслуживание
системы мониторинга.
При обобщении и централизации инфры для мониторинга расходы снижаются.
Но все-равно должен быть хотя бы один наблюдатель.
Наблюдать всякие дашборды может быть интересно и забавно,
но как только там надо искать ошибки - желающих мало.

* В Google создают простые и быстрые системы мониторинга.
С качественными инструментами для анализа состояния "по факту"
Гугль избегает "магических", которые самостоятельно исследуют
пороговые значения или причинно-следственные связи. ?Почему?
Исключение - правила, обнаруживающие нежданчики в запросах
конечных пользователей.
Долгосрочные закономерности - норм поддаются автоматическому анализу.

Если есть очень стабильные части системы - в гугле могут вводить разные правила.
Например: "Если дата-центр пуст, то не сообщать о его задержке отклика".
Некоторые команды вводят и сложные иерархии зависимостей.

Система мониторинга и алертинга должны быть просты для понимания всеми
в команде.
Оповещения тоже, простые, понятные, и только в случаях реальных (возможно грядущих)сбоев.

## Симптомы и причины

Система мониторинга должна отвечать на два вопроса:
* Что сломалось (симптом)?
* Почему (причина)?

### Примеры:

Симптом / причина
* Сервис возвращает 500 или 404. Сервер БД отказывает в соединении.
* Медленные ответы.  Процы перегружены или заломлен сетевой кабель.
* Пользователи не получаеют часть от CDN. Возможно их адреса попали в черный список IP.
* Личные данные видны всему миру. Что-то случилось с ACL, возможно при последнем обновлении.

## Черный и белый ящики

Черный ящик применяется редко, но метко.
Видит реально возоникшие а не спрогнозированные проблемы.

МБЯ зависит от возможности иссследовать систему изнутри (журналы, интерфейсы HTTP). Позволяет прогнозировать проблемы,
замаскированные рестартами сбои, и т.д.

Симптом и причина - понятия относительные. Медленная база - симптом для БД, но причина для сбоев в бэке и фронте.
Так что белый ящик может выдавать и симптомы и причины.

При сборке телеметрии необходим белый ящик.
Например веб-сервер медленно работает с БД. Может дело в БД,
а может в связи с БД.

Черный ящик - как источник алертов - норм, т.к. нет ложных тревог.

## 4 золотых сигнала (показателя)

LTES

* *Время отклика (Latency)* Важно делить успешные и неуспешные запросы.
* *Трафик (Traffic)* Нагрузка на систему в высокоуровневых &L7? единицах. HTTP RPS **отдельно для статического и динамического контента**.
Для потоковых видео/аудио - скорость передачи или кол-во параллельных сессий. Для k/v storages - transactions per second или кол-во возвращенных значений.
* *Уровень ошибок (Errors)* Кол-во (или частота) неуспешных запросов, либо явных (500),
либо неявных (200, но с битым контентом внутри) либо не соответствующих требованиям
Например SLO говорят, что запрос должен занимать не больше секунды. - Все что больше - ошибки.
?Коды ответов не могут отразить весь спектр? Могут понадобиться внутренние протоколы.
Если мониторить лишь балансировщик, будешь получать слабоинформативный 500,
а вот если мониторишь все можешь раскопать откуда он в итоге взялся.
* *Степень загруженности (Насыщение)(Saturation)* Для ограниченных ресурсов (память, IO диска, и т.д.) Многие системы теряют в производительности до того, как израсходуют 100% ресурсов.
Можно дополнить высокоуровневыми оченками нагрузки. Например, что будет при удвоении
трафика. Сколько % из доп. трафика обработается. А может вообще все тормознется,
и даже половина от общего трафика не обработается.
Все усложняется, если в зависимости от параметров - время выполнения запросов - разное. И приходится использовать косвенные показатели с известным верхним пределом.
Например *процент загрузки процессора* или *сети*.
Первый индикатор *насыщения* - увеличение времени отклика.
**Измерение 99го процентиля времени отклика за последнюю минуту** - говорит, что
намечается перегрузка.
Важно обращать внимание на прогнозы типа: "Похоже, ваша БД заполнит весь диск через 4 часа".

## Забота о "хвостах"

Средние оценки часто малопоказательны.
99% процентиль задержки у нас - может быть средней задержкой у какого-то клиента.
Гистограма распределения запросов в логарифмической шкале.

## Уровень детализации

* Контроль ЦПУ с периодичностью в минуту замажет пики.
* С другой стороны если заявлены 9 часов даунтайма в год, то пинговать сервер раз в 30 сек на предмет возврата 200 или проверять заполненность HDD может быть излишним.

Частые измерения - много инфы, дорого хранить.
Можно оптимизировать, накапливая данные с интервалом 1 сек, и сбрасывая на сервер хранения с интервалом в минуту.

* Записывать загрузку ЦПУ каждую секунду.
* Сделать динамическую гистограмму по 5%, где границы нагрузки для каждого столбца зависят от текущей нагрузки.
* Раз в минуту собирать итоговые данные.

## Просто, но не проще чем надо

Уровни сложности:
* Оповещения о достижении порогов в задержке отклика и разных процентилей для всех видов показателей.
* **дополнительный код** для обнаружения и отображения возможных причин.
* Информационные панели для каждой из возможных причин.

Типа если сильно усложнять систему мониторинга - её станет трудней поддерживать.

* Правила, определяющие реально аварийные ситуации должны быть простыми,
предсказуемыми, надежными.
* Если что-то используется слишком редко (реже раза в квартал) и редкими людьми, - от него м.б. можно избавиться.
* Если что-то собирается но не отражается на панелях и не используется в оповещениях и автоматизациях, - это кандидаты на удаление.

Заманчиво объединить мониторинг с кучей всего (профилирование, отладка, отслеживание исключений, анализ журналов, и т.д.). Но система станет сложной и менее стабильной.
Лучше иметь небольшие блоки с четким назначением.

### Правила при созданий правил мониторинга:

* Позволяет ли правило выявить *не обнаруживаемое иными средствами состояние*,
которое требует быстрой реакции и последствия которого уже заметны (или 100% скоро будут заметны) пользователю.
* Смогу ли я проигнорить это оповещение, зная что оно не критическое.
В каком случае и почему я проигнорю это сообщение? Как я могу избежать игнора.
Dzenly: Я так понимаю тут речь о том, что некритические оповещения тоже требуют реакции. Возможно лучше автоматом сразу тикеты заводить на них, чтобы не проигнорить.
* Понятно ли из оповещения, что уже есть негативное влияние на пользователей.
Отфильтрованы ли сценарии когда мониторинг реагирует на наши собственные действия (запуск нагрузочных тестов, отвод трафика), и пользователи не страдают.
* Могу ли я предпринять что-то в ответ на оповещение. Нужно ли реагировать
немедленно (например, ночью), или может подождать до утра.
**Можно ли безопасно автоматизировать действие. Будет ли это решение постоянным или временным.**
* Приходит ли это оповещение другим инженерам? Значит либо твое либо их - возможно зря отвлекают людей от работы.

### Позиция гугля по экстренным оповещениям на пейджеры (и мобильные телефоны видимо)

* Каждый раз я должен отреагировать мгновенно, но осмысленно. Если это будет больше нескольких раз за день - переутомлюсь.
* На каждое оповещение надо среагировать.
* Если среагировать может и автомат (т.е. не надо думать) - это не экстренное оповещение. ?странно? А что если автомата ещё нет ?
* Экстренное оповещение и вмешательство человека необходимы при появлении новой проблемы или ранее не встречавшегося события.
Dzenly: я так понимаю, это потому, что у них дежурный сразу старается фиксить систему,
чтобы подобной ошибки не было больше никогда.

Если все 4 пункта соблюдены - не важно черным или белым ящиком сгенерировано оповещение.
Лучше потратить время на поиск симптомов, чем на поиск причин. Причины
стоит искать самые явные и неизбежные. ?? Пока не понял почему.

## Долгосрочное наблюдение

По меняется, нагрузка меняется, SLO меняются.
М.б. на данный момент оповещение Н бывает редко, и ответ на него автоматизировать сложно, в дальнейшем оно может начать появляться чаще, и возможно
заслужит вложений в автоматизацию (??сделанную наспех??).
Тогда-то надо будет попытаться полечить root-cause, и если это не получится
- полностью по уму автоматизировать решение.

* Важно учитывать долгосрочные переспективы.
Каждый экстра-алерт отвлекает человек от улучшения системы.
Зачастую лучше отказаться от высоких SLO сейчас, чтобы улучшить
эти показатели в будущем.

### Пример с Bigtable: когда оповещений слишком много

У Гугля есть SLO для внутренней инфры.
Раньше SLO для Bigtable основывалось на синтетических средних показателей
для "хороших" клиентов. Из-за проблем в сервисе и в слое хранения даннных,
- худшие 5% запросов были значительнее медленнее средних.

При приближении к SLO - рассылались имейл оповещения, а при их превышении - экстра-алерты на пейджер.
Тратилось много времени на проверку, чтобы найти действительно требующие
срочных мер и из-за человеческого фактора, некоторые важные сообщения игнорились.
Многие оповещения были known-issues.

Было три этапа решения:
* Усилия по улучшению производительности BigTable.
* Взяли в SLO значение 75го процентиля времени отклика.
* Отключили имейл оповещения.

Т.е. переключились с тактических микрорешений на стратегическое.
Сервис улучшился.

### Пример с Gmail: предсказуемые ответы человека

У людей была проблема, и они спорили стоит ли делать костыль,
было мнение что из-за костыля не решится root-cause.
Такие автокостыли - красные флаги для менеджеров и других людей расставляющих приоритеты.

Важно смотреть загрузку команды оповещениями и тех. долг.

## Итоги

Оповещения важны.
Имейл - неэффективен.
Информационная панель важна.
Можно её соединить с журналом, чтобы анализировать динамику показателей.

Оповещения о симптомах и приближающихся проблемах, позволяют оптимизировать дежурства. Важно устанавливать достижимые SLO. И сделать возможность
быстрой диагностики через системы мониторинга.


# Эволюция автоматизации в Гугле

* Если у вас сильные автоматизаторы - осторожно, к силе нужна грамотная точка приложения. Автоматизация не в тех местах, может принести больше проблем, чем решить их.
* В идеале лучше создавать высокоуровневую автономную систему, не нуждающуюся
ни в автоматизации ни в ручной работе.

## Польза автоматизации

### Постоянство

Масштабирование - достаточная мотивация для автоматизации, но есть и другие мотивации:
* Управление группой компьютеров.
* Поддержка разного софта.
* Создание пользовательских учеток.
* Проверка создания бакапов,
* Устранение сбоев при отказах сервера,
* Манипуляция с данными (например изменение resolv.conf вышестоящих DNS и информации о DNS зонах).

Человек каждый раз выполняет одно и то же немножко по-разному, соответственно
человеческий фактор, ошибки, недостаточная надежность.
Основная польза автоматизации - воспроизводимость действий.

### Платформа

Если качественно проектируешь и имплементируешь автоматизацию, это
можно переиспользовать в других системах или для других задач,
плюс это приносит большой опыт во всем, что автоматизировал.

Лучше исправлять ошибку, чем обходить её каждый раз ручными действиями.
Нередко обучить людей выполнить какие-то действия сложнее, чем написать
автоматизацию.

Автоматизации не надо спать или долго соображать.

Платформа может экспортировать показатели производительности и предоставлять
детальную инфу о процессе.

### Быстрое восстановление

Автоматизация снижает MTTR (Mean Time To Repair) после ошибок.
Время людей освобождается для решения других задач.

Dzenly: Я так понимаю тут речь идет об автоматизации обнаружения проблем,
а не только об автоматизации их решения или закостыливания.

Чем раньше найдешь проблему (желательно вообще в прототипе), тем дешевле её поправить.

### Быстродействие

Автоматизация реагирует быстрее человека. В некоторых сервисах гугля столько
автоматизации, что никакой штат SRE не вывез бы её вручную.
Главное делать её грамотно, чтобы она не навредила системе.

### Экономия времени

Сравниваем усилия, сколько надо на написание автоматизации и сколько она сэкономит
человеко-часов.

Принимаем во внимание что запустить автоматизацию может любой, а значит мы отвязываемся от исполнителей и бас-фактора.


## Польза автоматизации для Google SRE

* Гугль видит минусы в использовании сторонних решений и пилит своё.
* Не надо жестко автоматизировать SRE для какого-то прототипа, который возможно
через месяц закроется.

## Применение автоматизации

* Автоматизация - создание программ для решения широкого круга задач. Мета-ПО,
программы, работающие с программами.

Цель создания автоматизации и задачи ей решаемые - могут отличаться.

### Примеры задач для автоматизации:

* Создание пользовательских учеток
* Влючение / отключение кластеров сервисов.
* Подготовка к установке или выводу из эксплуатации ПО или хардваря.
* Развертывание новых версий ПО
* Изменение конфигов ПО
* Изменения согласно зависимостям (особый случай изменения конфигов).
* и т.д. до бесконечности.

### Применение автоматизации в Google SRE

В Гугле есть все перечисленные выше автоматизации.
SRE в первую очередь стремится поддерживать инфраструктуру,
а не качество проходящих данных.
Например, ставим обновление, пропала половина данных.
Гугль пилят оповещения обо всех масштабных изменениях данных.
Но вряд ли SRE будут писать программу, меняющую содержимое учеток системы.
*Dzenly: Непонятно что хотели сказать. Имеются в виду учетки БД ? Т.е. написание миграций за разработчиков ?*
Автоматизация для SRE важна как средство управления жизненными циклами систем,
например, развертыванием сервиса на новом кластеер, а не как средство управления
их данными.

Puppet, Chef, cfengine, Perl, доступны многим. Какие-то низкоуровневые,
какие-то из коробки дают многое, но имеют свои недостатки.
Например, мы часто считаем, что отправка нового файла программы - атомарная операция:
т.е. в итоге в кластере либо останется старая или появится новая версия.
Однако в реальности все сложнее и система может попасть в нестабильное состояние
(сбои в сетях, в хардваре).
Соответственно новые бинарники могут попасть в stage, но не попасть в прод (staged but not pushed. Dzenly: или они имеют в виду закоммичены но не запушены? Да не, врядли. Stage это ещё даже не коммит.),
или попасть в прод но без рестарта, или перезапущены, но не доступные для проверки.
Немного абстракций могут это все разруливать, они либо прекращают работу
и требуют вмешательства инженера, либо вообще ничего не делают (делают вид что всё ок?).

В гугле все сложно, и как правило применяются инструменты без сильных абстракций.

### Уровни автоматизации

В идеальном мире лучше иметь систему, не нуждающуюся в стыковочной логике.
Внутренняя реализация более эффективна.
Нужно выделить ситуации использования стыковочной логики, как правило это
первоочередные действия, типа добавления учеток или запуска системы,
и поискать способы сделать это из самой системы.

Внешним автоматизациям приходится поддерживать консистентность с системой,
когда она меняется. Это все требует тщательного планирования работ и расстановки
приоритетов. Также надо тестировать развертывание после каждого изменения.
Критически важная, но редко исполняемая (а значит и тестируемая) часть автоматизации
зачастую оказывается особенно уязвимой.
Например - восстановление после отказа кластера - редко выполняемая автоматизация (может раз в несколько месяцев).
И если спецом не тестить, - система и автоматизаци могут разъехаться.

#### Путь развития автоматизации

* Нет автоматизации. Восстановление БД после отказа вручную на всех площадках.
* Управляемая извне автоматизация, характерная для данного типа систем.
SRE создал сценарий для восстановления в своей домашней директории.
* Управляемая извне общая автоматизация. SRE добавил поддержку восстановления
в общий сценарий, который юзают все.
* Управляемая изнутри автоматизация, специфичная для данного типа систем.
БД поставляется с собственным сценарием восстановления.
* Системы, для которых автоматизация не требуется. БД сама обнаруживает проблемы,
и автоматом восстанавливается.

Есть ещё автоматизация, которая затрагивает несколько систем.
Например, изменения в цепочке серверов Chubby, или флагов в клиентской либе Bigtable,
для повышения надежности доступа.
Вручную вносить их запаришься. Даже если это просто "перезапустить и проверить".

## Автоматизируе все и исключаем себя из процесса

Юзали MySQL, автоматизировали почти всю рутину по реплицированию.
И тут захотели мигрировать MySQL на Borg. Чтобы Borg обрабатывал запуск/перезапуск аварийно завершившихся задач. И чтобы была возможность запускать в контейнере несколько экземпляров MySQL.
Но встретили сложность, Borg двигает свои таски пару раз в неделю.
Для реплик это норм, а вот для мастеров - нет.

Файловер мастера длился 30-90 минут на инстанс.
Потому что мы работали на общих машинах, и вынуждены были ждать регулярных апдейтов ядра, в добавок к обычной частоте отказов машин нам пришлось учитывать
какое-то кол-во несвязанных отказов каждую неделю.
Это в сочетании с шардированием означало что:

* Ручные файловеры съедали существенные ресурсы человеков и не давали поднять availability более 99%, что было мало для бизнес требований к продукту.
* Чтобы выполнить наш error budget, каждый файловер должен был занимать не более 30 сек даунтайма. Если это зависит от человека, то в 30 сек уложиться невозможно.

Они написали свой автоматический демон для файловеров - Decider.
Он укладывался в 30 сек в 95% случаев и MySQL on Borg стал реальностью.
Для этого пришлось добавить логику обработки отказов в приложение,
т.е. кастомизировать JDBC. Но в итоге нагрузка на человеков уменьшилась на 95%.
Освободилось время для другой работы, и они наавтоматизировали ещё много чего,
и освободили 60% машин.

### Автоматизируем процесс запуска кластера

Жила-была команда. И с каждым запуском нового кластера в неё нанимали
доп. человека.
Для подготовки кластера выполнялись след. действия:
* Оборудование дата-центра питанием и охлаждением.
* Установить основные коммутаторы и настроить соединения с объединяющей магистралью.
* Установить несколько первых стоек с серверами.
* Сконфигурять базовые сервисы, типа DNS и установщиков, затем сконфигурять
сервисы блокировки, хранения, вычисления (это оказалось сложно).
* Развернуть остальные стойки серверов.
* Распределить ресурсы сервисов, с которыми работают пользователи,
чтобы команды инженеров настроили эти сервисы (и это оказалось сложно).

Сложно было потому, что подсистемы хранения и вычисления были в стадии разработки
и часто менялись.
Сервисы имели больше сотни подсистем-компонентов,
и каждая имела сложную сеть зависимостей. Неверная конфигурация одного из компонентов,
могла привести к сбоям у пользователей.

Многопетабайтный кластер Bigtable по соображениям быстродействия
был сконфигурен, чтобы не использовать первый (журналирующий) диск из 12.
Год спустя программа автоматизации решила, что если первы диск не юзается,
то и остальные тоже и уничтожила все данные. Благо были бакапы.

### Выявление несоответствий сп помощью Prodtest

Кол-во кластеров растет, кол-во ручных настроек растет.
И все больше было конфликтов между флагами.
Баш скрипты особо не масштабировались ни от кол-ва людей, желающих внести изменения,
ни в зависимости от общего кол-ва изменений, нужных для кластера.
Также скрипты не проверяли:

* Доступны ли все компоненты, от которых зависит сервис, и корректно ли они сконфигурированы?
* Согласуются ли все конфиги и пакеты с другими конфигами и пакетами?
* Может ли команда подтвердить, что все исключения, сделанные при конфигурации,
осознанны?

Они написали на Python движок Prodtest, для юнит(Dzenly: скорее интеграционного),
тестирования цепочек зависимостей. С визуализацией графов выполнения тестов.
Prodtest расширялся на основе кейсов с ошибками.

Менеджеры смогли прогнозировать время на пуск кластера, и начали понимать
на что тратятся полтора месяца для перехода кластера из состояния "готов к работе"
в состояние "реально работаю с реальным трафиком".
Но SRE получили задание - через три месяца 5 кластеров будут "готовы к работе",
запустите их в течение недели.

### Идемпонентное разрешение несоответствий

Чтобы решить задачу запуска за неделю, они перешли от парадигмы:
* Юнит тесты на Python исплользуются для поиска несоответствия конфигураций
к парадигме
* Python код применяется для исправления несоответствия конфигурации.

Т.е. они объединили тесты с исправляющим кодом.

### Движение к специализации

#### Характеристики автоматизации

* *Компетентность* - способность отдавать корректные, точные результаты.
* *Задержка* - быстрота выполнение шагов инициализации.
* *Релевантность* - доля процессов с автоматизацией.

Запускаетя и обслуживается владельцами сервиса - высокая Компентентность.
Высокая задержка - владельцы сервисов выполняли процессы в свободное время или поручали их новым инженерам.
Высокая релевантность - Владельцы сервисов знали когда меняется ситуация в
мире и могли исправить решение по автоматизации.

Но все меняется, люди потеряли экспертизу. Тесты стали неактуальными.

* Команда, по ускорению текущего процесса ввода кластера в строй,
не заинтересована снижать техдолг владельцев сервиса, которым выпускать его в промышленную эксплуатацию.

* Если сам не юзаешь автоматизацию, то и нет стимула пилить систему, которую легко автоматизировать.

* Если плохая автоматизация не влияет на расписания продакт менеджера,
он будет отдавать предпочтение новому функционалу.

**Наиболее функциональные инструменты обычно создаются людьми, работающиими с ними.**
Поэтому и разработчикам важно наблюдать за своими системами в промышленной эксплуатации.

Процесс ввода кластеров в строй стал некомпетентным, долгим, неточным.
Dzenly: ? Дальше абзац про то как они настроили права доступа, и это каким-то
боком их спасло. Возможно аудит их спас ?

### Запуск кластера, ориентированный на сервисы

SRE перешли от написания shell скриптов в домашних каталогов к построению взаимо
просматриваемых RPC серверов с подробными ACL.

SOA (Service Oriented Architecture).  Владельцы сервиса должны
отвечать за создание экземпляров демона Admin Server, для обработки RPC
по запуску и останову кластера, к которым обратится система,когда
кластеры перейдут в состояние готовности.

Каждая команда (разработки?) предоставляла контракт API, небходимый для автоматизации
ввода кластера в строй. И под капотом могла менять реализацию как хочет.

## Borg: появление компьютера размером с дом

Стойки с оборудованием.
Вход на одном из хорошо известных мастер-компов для выполнения админских заданий.
Эталонные копии бинарнников и конфигов лежали на этих машинах.

В общем история про то, как они построили кластер машин, с самовосстанавливающимися машинами.

## Основное качество - надежность.

Автоматизация работает, люди перестают в ней шарить. Автоматизация дает сбой
и некому чинить.
Автономность систем - все-равно хорошо.

### Сбои при масштабирования

Автоматизация замен стоек и очистки дисков.
Однажды был сбой после очистки.
DiskErase увидел что список пустой, и интерпретировал это как "все машины",
и запустил удаление всех машин с CDN.
Два дня доводили до ума CND, и пару недель пилили идемпотентность в процесс списания машин.

===========================

# 8. Технология выпуска ПО

TODO: до части III



===========================

# III. Практики

Пирамида Маслоу для работоспособности системы.

# Мониторинг

## Реагирование в критических ситуациях

Дежурства это ещё и способ узнать насколько хорошо все работает.

Иерархия:

* Наблюдение
* Реагирование на инциденты
* Постмортем анализ
* Тестирование + процедуры новой версии
* Планирование мощностей
* Разработка
* Продукт

Не обязательно прям сразу исправлять проблему раз и навсегда.
Сначала имеет смысл остановить кровотечение.
Например, временно отключив какой-то функционал, или перенаправив
трафик на другой экземпляр сервиса.

## Постмортем и анализ root-cause

Гугль стремится настроить систему так, чтобы получать оповещения
**только по поводу новых** и действительно существенных проблем.

## Тестирование

Как только мы поняли, что что-то начинает идти не по плану, - следующий шаг -
предотвратить инцидент. Предотвратить болезнь лучше чем вылечить когда она появилась.
Тесты дают какую-то уверенность, что каких-то классов ошибок у нашего софта нет.

## Планирование мощностей

У Гугля есть Auxon, - тулза для автоматического планирования мощностей.
Лоад балансинг обеспечит правильное использование наших мощностей.

## Разработка

Проектирование крупномасштабных систем и инженерная работа внутри организации.
Целостность данных.

## Продукт

Наверху пирамиды надежности - рабочий продукт.

## Другие источники от Google SRE

Процесс тестирования хрупок, и ошибки в нем - могут сильно повлиять на общую стабильность.

Тут у них ссылки на:
* тестирование устойчивости,
* планирование производительности,
* безопасность

==============

# 10 Оповещения на основе данных временных рядов

Благодаря мониторингу:
* Владельцы могут понимать влияние изменений на сервис,
* Грамотно реагировать на критические ситуации,
* Обосновывать необходимость самого сервиса, оценивать насколько он соответствует
бизнес-целям.

Мониторинг крупной системы сложен:
* Огромное кол-во компонентов
* Ограниченные ресурсы инженеров, + их можно грузить не на 100%.

Важно замерять распределение допустим, задержки среди всех серверов в данном регионе.
Тогда можно понять какие факторы влияют на задержку.

В крупномасштабных системах неприемлимо посылать оповещения обо всех сбоях на всех машинах.
Нужно создавать приложения устойчивые к сбоям.
Крупная система должна собирать сигналы и отсекать ненужное.
Т.е. система мониторинга должна алертить о самых важных показателях сервиса,
но при этом иметь детали, чтобы если что поисследовать отдельные компоненты.

В старой системе снимались показания (через выполнение пользовательских сценариев) и рассылались алерты.
В новой - собираются временные ряды, и есть мощный язык для преобразования временных
рядов в графики и оповещения.
Я так понимаю в этом BorgMon из коробки много чего идет.

## Укрепление позиций Borgmon

Prometheus рулит, и похож на Borgmon.
Не надо по всяким ssh куда-то лазить, все из одного места.
Мониторинг методом белого ящика.

Данные копятся, анализируются. Шлются оповещения.

```
curl http://webserver:80/varz
http_requests 37
errors_total 12
```

Крупные сервисы разбиваются ниже уровня кластера на множество
экземпляров-скрапперов. Они наполняют данными экземпляр,
работающий на уровне кластера.

## Инструментарий для приложений

```
http_responses map:code 200:25 404:0 500:12
```

## Сбор экспортированных данных

Для поиска объектов экземпляр Borgmon настраивается
согласно списку целей.
С использованием одного из методов разрешения имен.
Раз в какой-то интервал опрашивает разные varz с сервисов,
и расширяет коллекцию для каждого экземпляра на весь интервал.

Также пишутся мета данные:

* Отрезолвилось ли имя в хост и порт.
* Ответил ли объект на запрос о сборе данных.
* ответил ли объект на проверку работоспособности.
* в какое время завершился сбор данных.

Всех этих данных хватило для написания правил
для проверки доступности наблюдаемых задач.

SNMP разработан, чтобы соответствовать мин. требованиям для передачи данных и продолжать работать, когда другие сетевые приложения
подыхают.

HTTP для скраппинга противоречит этому принципу,
но это редко становится проблемой.
Dzenly: ?Т.к. часто симптомов достаточно, и не обязательно копать сильно вглубь?

Система разработана устойчивой к сбоям, и эти сбои можно юзать в правилах.

## Память временных рядов как хранилище данных

Borgman хранит много чего в оперативке и время от времени сбрасывает на диск.

12 часов - временной горизонт (интервал между самой
свежей и самой старой записями).

Метки.
Как минимум:

* var, job (задача), service (коллекция задач), zone (местоположение, обычно дата-центр).

...

Можно добавлять разные метки.
Ну и дальше рассказывается про разные плюшки, типа что можно запросить
за интервал времени. Вроде это все есть в прометее.

## Вычисление правил

Централизованно, в Borgmon.
Конфиги в одном месте.
Можно запускать правила параллельно, можно последовательно.
Например, если медленные запросы - можно следующим последовательным
правилом - дернуть метрики по вычислительным ресурсам.

Агрегирование. Общая интенсивность запросов к job - сумма
интенсивности изменений всех тасок.

Total QPS rate джобы в датацентре это сумма
всех скоростей изменения всех каунтеров.

счетчики - только увеличиваются (пройденные километры).
индикаторы - могут иметь любое значение (кол-во топлива, или текущая скорость).
Счетчики сохраняют историю, индикаторы нет (инфа между запросами данных теряется)

Например, для веб сервера хотим добавить алерт, при превышении
процента ошибок над SLI.
Т.е. когда сумма частоты не 200 кодов по всем таскам в кластере,
деленная на сумму частот запросов всех тасок в кластере
больше чем сколько-то.

### Это можно сделать так: (стр 164)

* Собираем коды ответов и размещаем их в векторах рейтов на текущий момент времени. Для каждого кода - свой вектор.

* Считаем total error rate как сумму этого вектора, и
выводим на экран единственное значение для кластера.
200 тут нет, все про ошибки.

* Дальше делим рейт ошибок на рейт общлих запросов. Одно значение
в данный момент времени.

И мы можем посмотреть хронологию изменения уровня ошибок.

Какие-то из последних перед инцидентом попыток забора данных могут
дать сбой.

В общем надо уметь делать всякие запросы promQL like.

### Оповещение

Если некоторое правило срабатывает, не обязательно слать сообщение
прям сразу.
Нужно подождать какое-то время. Вдруг вылечится.
Т.е. какие-то секундные выбросы - норм.

Например, уровень ошибок в течение 10 мин превышает 1%.
А общее кол-во ошибок превышает 1.

Шаблон для сообщения.

Центральный сервис - AlertManager.
Получает RPC с оповещениями, когда срабатывает правило,
а затем ещё раз, когда оповещение отправляется.

Можно наводить логику:
* Задерживать некоторые оповещения, если активны другие.
* Дедуплицирование оповещенией.
* Разветвление входных и выхдных оповещений.

* Критические - дежурным.
* Важные, но не критические - в очередь тикетов.
* Все остальные - **справочники для информационных панелей**.

### Разбиваем топологию системы мониторинга на части

Один общий сборщик статистики - точка отказа.
Типичный деплой использует два и более глобальных Borgmon
для высокоуровневого агрегирования и один экземпляр
в каждом ДЦ для наблюдения за задачами этого ДЦ.

Можно разбивать на уровни скрапинга и агрегирования (тут вычисляются правила).
Иногда скрапинг ограничен мощностью железа.
Глобальный уровень можно разбивать на уровень правил
и уровень инфопанелей.
Высокие уровни могут фильтровать потоки от низких.
Иерархия агрегирования собирает локальные кэши релевантных временных рядов.

### Черный ящик

Белый ящик не дает представления со стороны пользователя.
Если запрос не дошел до целевого сервиса - мы его не заметим.
Например, у пользователя сломался DNS.

Инструмент **Prober** - .
Проверяет работу протокола до цели и репортит ок или не ок.
Может слать алерты прям в алертменеджер, или предоставлять
varz для Borgmon.
Может валидировать payload ответа протокола (HTML контент HTTP ответа, e.g.)
валидировать что контент ожидаем, и доставать и экспортировать
значения как Time Series.
Команды часто юзают Prober для просмотра гистограм
времени ответов по типам операций и размеру пейлоада.

Prober может быть натравлен на фронт или после лоад балансера.

### Поддержка конфигов

Конфиги Borgmon отделяют определение правил от наблюдаемых объектов-
целей.
Т.е. можно эти же правила применять для разных объектов.

Шаблоны языков для правил. Можно создавать библиотеки.
Можно создавать регрессионные и модульные тесты для правил.
И даже есть CI/CD для прохождения всех тестов и отправления
правил на экземпляры Borgmon.

Классы конфигураций:
* Первый: кодирует схему переменных экспортнутую из данной либы кода,
так, что любой юзер либы может переиспользовать этот
шаблон его varz. Такие шаблоны есть для HTTP сервера, памяти, дисков,
RPC сервисов.
varz вроде бы бесхемный, но в либе схема появляется.

* Второй: управляет агрегацией данных из односерверной таски на
глобальные сервисы.
Общие правила агрегации, которые инженеры могут юзать для моделирования
топологии сервиса.
Например, сервис может обеспечивать один глобальный APi,
yj быть расположенным в нескольких ДЦ, и там будут шарды, а в шардах джобы, в джобах - таски.
Инженеры могут моделировать эту иерархию,
чтобы изолированно отлаживать свои куски.
Все компоненты что-то шарят.
Таски - конфиги. Джобы в шарде - ДЦ, физические сайты - сеть.
Это все дает свои метки в данных.

### 10 лет спустя

Затраты на обслуживание должны расти медленее системы.

===============

# Быть на связи

Оперативная поддержка OPS.
SRE отличаются от OPS тем, что подходят к решению проблем со стороны
разработки.
Т.е. операторы без разработчиков вряд ли найдут решение.
Поэтому SRE должны уметь программировать и проектировать программы.
Поэтому не менее 50% они должны посвящать разработке: усовершенствование сервисов, и средств автоматизации для повышения
эффективности работы команды.

## Жизнь дежурного инженера

Должен быть доступен для выполнения операций за считанные минуты.
Допустим 5 минут для сервисов, с которыми пользователи взаимодействуют
непосредственно или иных критических сервисов.
И 30 минут для менее чувствительных к времени систем.
Нередко компания предоставляет служебный телефон и имеет гибкую систему
доставки оповещений.

99.99% - 13 минут простоя за квартал.

### Алерт
*Дежурный подтверждает получение*
*Ранжирует проблему по приоритету*
*Приступает к решению*, при необходимости привлекая к этому
других членов команды.

### События
* Низкоприоритетные оповещения.
* Появление новых версий ПО.
тоже могут быть обработаны дежурным в рабочее время.

Первая и вторая очередь дежурства.
Где-то вторая обрабатывает несрочные уведомления.
А где-то подключается если не справляется первая.

Иногда задачи могут передавать в другие команды.

### Сбалансированная организация дежурств

Кол-во дежурных вычисляется через процент рабочего времени
на дежурства.
Качественный состав - по кол-ву инцидентов за смену.

SRE менеджеры должны это балансировать.

#### Баланс кол-ва

* 50% - разработка.
* 25% - срочные работы.
* 25% - другие виды работ, не связанные с разработкой проектов.

Рассчет кол-ва инженеров для 24/7. Четверть времени.
Допустим 2 человека всегда на дежурстве (1 и 2 очереди).
Смена - неделя.
3 недели после этого человек занимается проектами и другими задачами.
Соответственно период смены - 4 недели.
4 недели по 2 человека - 8 человек.

Если команда разбита на две площадки.
То в каждой части должно быть по 6 человек.
Как для 25%, так и для критической массы в каждой команде.

Площадки - это что-то имеющее разные часовые пояса.

Несколько площадок с разными часовыми поясами хорошо, т.к. можно работать только днем.
Ограничение дежурных позволяет инженерам лучше знать систему.

Но распределенные команды - это коммуникационные и координационные издержки.

#### Баланс качества

* Должно быть время на обработку инцидентов и на написание постмортемов.

* *Инцидент* - последовательность событий и оповещений, которые имеют
одну root-cause и будут отражены в одном отчете.
* В среднем на выяснение причин, предотвращение последующих инцидентов и написание отчетов уходит 6 часов.
* 12 часовая смена - два инцидента максимум.
* Инциденты должны быть распределены стационарно, максимально равномерно, с ожидаемой медианой равной 0.
* Т.е. не должно быть нормой иметь стабильно 1 инцидент в день.
* Т.к. в следующий день прилетит нежданчик и будет уже 2 инцидента.
* Если лимит временно превышен, например, по итогам квартала - необходимо вернуть нагрузку к допустимой величине.

#### Компенсация

Гугль дает отгулы, или прямую денежную компенсацию.
Но регулирует, чтобы люди не брали на себя много и не выгорали,
или не переставали заниматься инженерной деятельностью.

#### Чувство безопасности
Дежурный берет на себя ответственность за то, с чем сталкивается юзер,
за доход от важных систем и т.д.
Хорошо думать и быстро действовать - жизненно важно.

Два вариант действий (как раз Каннеман):
* Интуитивное и немедленное реагирование.
* Рациональный подход.

Гормоны стресса приводят к немедленному и неправильному реагированию.
В частности к некорректным обобщениям.

Пока не посмотрели данные - не строим догадки. Интуиция может подвести.

Ресурсы для работника:
* Хороший менеджмент
* Хорошо определенные процедуры обработки инцидентов
* Безобвинительная культура написания отчетов

Команды разработчиков систем, поддерживаемых SRE инженерами,
обычно также участвуют в круглосуточных дежурствах,
и всегда можно эскалировать к этим командам-партнерам.
И это нормальная практика.

Если понимаем что проблема достаточно сложна, и нужно привлекать
несколько команд, или есть большая неопределенность сколько займет решение, - уместно применить официальный протокол обработки инцидента.

Протокол - веб приложение. Где тебя ведут по шагам.
Приложение позволяет менеджеру сосредоточиться на решении проблемы,
и не тратить время
на рутину, типа имейлы, или прямое общение.

Важно также понять что пошло не так, а что пошло норм.
И предпринять действия, чтобы избежать этих ошибок в будущем.

По значительным инцидентам - постмортемы с полной хронологией.
Без упоминаний людей,только факты по событиям.

## Избегание неуместной нагрузки

Что если превысим 50% лимит на операционку.

### Операционка

Команда и начальник отвечают за равномерность нагрузки на команду.
Если временно отдолжить опытного SRE, чтобы справиться с проблемами.

Нагрузку надо измерять. Например, норма, это менее 5 несрочных запросов в день, и менее 2х экстренных.

Причиной перегрузок может быть сильно спамящий мониторинг.
Экстренные оповещения должны быть только по тем событиям,
которые нарушают работоспособность и SLO.

Необходимо чтобы все оповещения можно было обработать.
Много оповещений с низким приоритетом притупляют внимания,
и приводят к пропускам экстренных оповещений.

Важно контролировать кол-во оповещений по одному инциденту.
Взаимосвязанные оповещения должны группироваться.
Чтобы они не смотрелись как разные инциденты.
Нужно настраивать систему оповещений чтобы по 1 инциденту было одно оповещение.
Фильтры на ненужные оповещения.

Иногда дело в самом вашем софте. Тут надо работать с разработчиками.

Если сам софт совсем нестабилен - отдать его временно на поддержку
разработчикам, пока не доведут до ума. :)

### Коварный враг: операционная недонагрузка

Оторванность от промышленной эксплуатации приводит либо
к излишней уверенности либо наоборот к неуверенности.
Каждому инженеру надо подежурить, раз или два за квартал.
Плюс упражнения "Колесо неудачи".
DiRT (Disaster Recovery Training)

============

# 12 Эффективная диагностика и решение проблем

Можно научиться решать проблемы. Это не генетические таланты.

Тем не менее, понимание работы системы эффективней знания
общепринятых подходов к траблшутингу.

## Теория

Гипотетико-дедуктивный метод.
Последовательно выдвигаем и проверяем гипотезы о причинах ошибки.

* Изучить отчет об ошибке.
* Проверили результаты телеметрии (varz) и журналы системы.
* Дальше комбинируем подходы:
** изучать текущее состояние в поискать доказательств или опровержений гипотезы
** изменять состояние, чтобы посмотреть что будет.
* Когда нашли проблему - лечим её.
* Дальше ищем root-cause.
* Пишем отчет.
* Устраняем root-cause.

Изначально неправильно выбранная гипотеза - основной пожиратель времени.

* Нужно экономить время изучая только то, что относится к делу.
* Нужно шарить в системе, чтобы понимать что в ней можно безопасно крутить для проверки гипотез. И какими входными данными можно что-то потестить.
* Сильно невероятные гипотезы могут съесть время.
* Сильно простые - тоже (например, что тут дело во вчерашней ошибке).
* Предположение о том, что есть какие-то зависимости, которые окажутся
случайностями - съедят время.

Важно знать общепринятые паттерны для распределенных систем.
Важно помнить что не все ошибки равновероятны.

В хорошо спроектированном кластере файловых систем,
задержки из-за неисправности дисков крайне маловероятны.

Некоторые события могут казаться взаимозависимыми, но
на самом деле они зависят от чего-то третьего.
Например, при сбоях питания будут и диски барахлить и сеть.
Чем масштабней система, тем чаще будет казаться что есть зависимости.

## Практика

### Отчет об ошибках

Это может быть богатый сгенеренный автоматом отчет,
либо коллега просто сказал "система стала медленней".
Отчет должен содержать описание ожидаемого поведения,
реального поведени,
и если возможно - способа воспроизведения косяков.

Отчеты могут храниться в системе баг-трекинга.
И быть написаны в едином стиле.

В гугле есть разные веб-формы, которые помогают создавать отчеты.

Хорош бы сюда добавить автоматизацию для диагностики,
а может даже и для исправления проблем.

Если без всяких трекеров передавать отчеты человеку,
то обычно передают знакомым, и и нужно доводить такие отчеты до ума.

### У Шекспира проблема

Получили ошибку
ShakespeareBlackBoxProbe_SearchFailure.
Результаты для определенного запроса не были получены в течение
5 минут.

Система оповещения заполнила баг. Снабдила его ссылками
на результаты мониторинга.

Получили проблему - надо понять что с ней делать.
Отранжировать её по степени опасности:
* некоторые могут влиять только на одного пользователя с очень
специфическими условиями.
* некоторые могут завалить весь сервис.

Если эта серьезная проблема у нас возникла впервые,
можно мобилизовать всех инженеров.
Но если это давно известный дефект.

Оценка опасности требует опыта в проектировании,
рассудительности, умения сохранять спокойствие.

Может захотеться сразу искать проблему. Не делайте так!
Нужно хотя бы частично возобновить работу системы.

* М.б. перенаправить трафик.
* М.б. ограничить трафик, что у пользователей будет притормаживать но работать.
* Может быть у сервиса есть не сильно важные системы,
которые кушают ресурсы. И их можно временно отключать.

* Сохранить записи из журналов, для последующего анализа.

* Если есть риск потери информации, - возможно лучше остановить всю систему. Или переключить её в read-only состояние.

### Обследование

Должна быть возможность исследовать что происходило с каждым компонентом
системы и установить правильно ли он работал.

*Первый помощник - мониторинг* с записью хронологии показателей системы
(time series).

*Второй - логирование*. От одного или множества процессов.
+ Трейсинг через весь стек.

Текстовые логи позволяют быстро найти проблему.
Бинарные логи - хороши для ретроспективы.

Если трафик большой, можно семплировать одну из 1000 операций.

Хорошо если есть язык выборки. Мол дай мне операции, которые
соответствуют условию такому-то.

Например, которые больше скольки-то байт пейлоада,
или которые длятся больше стольки-то милисекунд.


*Третий - экспозиция текущего состояния*
Гугль серверы имеют эндпоинты, которые показывают примеры
последних
полученных и отправленных RPC.
И так можно понять кто с кем коммуницирует, без всяких архитектурных диаграмм.

Эти эндпоинты, показывают error rates и latency для каждого типа RPC.

Линки с черноящикового мониторига показывают,
что пробер посылает попытки найти этот шекспировский текст.
Ожидает 200 JSON пейлоад с результатом.
Шлет раз в минуту последние 10 мин.
Половина проб успешны, но нет различающегося паттерна.
Пробер, к сожалению, не показывает что возвращается (коды ошибок),
если он фейлится. Делаем пометки - пофиксить это в будущем.

Мы руками curl к эндпоинту поиска, и видим 502 без payload.


Хорошо знать систему, это помогает выдвигать наиболее вероятные гипотезы.
Но есть и ряд общих подходов:

* *Упрощать и сокращать*
Есть компоненты, задача которых принимать запросы и отдавать результаты.
Есть связи между компонентами.
Задаешь известный запрос, на который должен быть известный ответ.
Когда добьешься воспроизводимого тест кейса -
дебаг становится веселее.

Если ещё этот кейс вынести в тестовую среду, чтобы иметь большую
свободу в издевательствах над системой - вообще хорошо.

Идет от одного конца стека к другому и проверяешь все ли ок.
Если прям совсем большая цепочка - можно пойти бисекцией.

#### Спрашивай что, где, и почему?

Сломаная система все-равно что-то пытается делать, только не то, что тебе надо.

Можно понять *что* она делает.
И спросить *почему*.
И *где используются ресурсы*.
и *куда направляются результаты*
(и входные данные тоже)

### Раскрытие причины симптомов

* Spanner cluster имеет высокую латентность, и RCP к его серверам
падают по таймаутам.

* Почему? Spanner таски съели все CPU.

* Куда делись ресурсы ? Они сортируют какие-то логи на диске.

* Где код, сортирующий логи юзается?
Когда пробегаем рэгэкспом по путям в лог файлах.

* Решение: переписать регулярное выражения, чтобы не юзать обратные ссылки.
Подумать о новой системе RE2.

Анализ последних изменений в системе - тоже хорошая отправная точка.
В хорошо спроектированной системе
должны логироваться любые изменения в деплойментах и конфигах.
От бинарников по обработке трафика до системных пакетов,
установленных на нодах.

Поиск связи между изменениями в системе и другими
событиями в системе, может быть полезной в
конструировании бордов для мониторинга.
Т.е. как я понял борда мониторинга сразу отражает и логи по изменению конфигов и
бинарников.

Например, графики по system error rates могут обогащаться данными
по началу и концу деплоя новой версии.

То, что мы запросили curl по api/search и получили список бакэндов,
и там встретили ошибку 502 - сразу отсекло, что дело не
во фронтэнде и не в балансировщике.

#### Норм разрабатывать тулы для диагностирования сервисов.
Гуглевские SRE тратят много времени на такие тулзы.
Такие тулзы конечно специфичные, но старайся DRY.

### Тестируй и лечи

Компоненты. Сеть. Файрвол.
Имитируешь флоу движения данных - проверяешь что мешает.

Простые тесты - пинги.
Сложные - отключение трафика в кластеер
и ввод специальных запросов для имитации рейс кондишнов.

* Идеальный тест должен содержать взаимоисключающие альтернативы.
Т.е. чтобы подтвердить одну группу гипотез и опровергнуть другую.

* Выполняй тесты чтобы проверять сначала высоковероянтые гипотезы.
Учитывай риски для системы от твоего тестирования.

Т.е. допустим сначала что есть сетевое соединение. А потом
что недавнее изменение в конфигурации удалило доступ для второй
машины.

* Эксперименты могут уводить в сторону от решения, из-за смешения факторов.
Например файрвол может разрешать доступ к БД только с определеного IP.
Соответственно ты пингуешь ДБ и думаешь, что она отвалилась.
Наверное лучше пинговать её с той же машины.
Или посмотреть прямо в ней коннекшны.

* Текущие тесты могут влиять на результаты следующих.
Выдача процессу больших CPU может сделать
операции быстрее, но и вызывать рейс кондишны.
Включение вербозных логов - проблемы с латентностью ухудшатся.

* Некоторые тесты неточны, а лишь предполагают.
Сложно сделать воспроизводимый тест на рейс кондишн или дедлок.
Поэтому иногда стоит удовлетворяться менее точными экспериментами.

Всегда записывайте какие идеи вы имели.
Какие тесты выполнили и какие результаты увидели.
Особенно когда имеете дело со сложными и затяжными кейами,
документация будет критической для вспоминания
что делал, чтобы не повторять то, что уже проверил.
Это также поможет подключиться другим людям.

Также помечайте какие изменения внесли в систему, чтобы потом откатить
если что.

## Отрицательные результаты волшебны??

Отрицательный результат - это когда не получили что ожидали.
Думал что-то в системе улучшится, но нет.

Отрицательные результаты надо документировать.
Это будет подтверждать или опровергать что выбранный дизайн хорош.

Например хотели юзать технологию, но она не подошла, надо
поделиться этой инфой.

Также эта инфа поможет делать эффективней следущюие эксперрименты.
Стабильно повторяющийся отрицательный результат может быть
важен для разработчиков и архитекторов.

Инструменты сделанные для эксперимента могут переиспользоваться дальше.
Тестирование нагрузки например.

Копите знания в общем. Для будущих архитектур и доработок.

Качественные детальные отчеты. Опубликованные.

Если в каком-то конструкторском документе все гладко и не упоминается
о неудачах в опробации дезайн десижнов, отнеситесь к нему
скептически.

### Лечение

Свели причины к одной.
Надо доказать что это та самая причина.
Допустим искусственно воспроизвести ситуацию.
В проде - может быть сложно. М.б. можно на тестовой среде.

* *Сложность систем* М.б. комплексное сочетание факторов.
Состояние (возможно редкое), для которого можно воспроизвести проблему.
* Воспроизводить на проде часто невозможно.
Если есть копия - можно трениться на ней.

Нашли все факторы - задокументь.
Полный постмортем и предложение как улучшить, чтобы такого не повторилось.

## Реальный пример.

App Engine - PaaS
Позволяет создавать свои SaaS?

Клиент говорит, что увеличилась задержка, использование ЦП,
и кол-во процессов для обработки трафика в их приложении.
Система для создания документации для разработчиков.

В коде изменений требующих больше ресурсов нет.
Трафик не увеличился.

Было подозрение что дело в App Engine.

Обычно задержка происходит при увеличении трафика,
или при изменении конфигурации.

Был всплеск трафика, и повысилось потребление ресурсов,
но оно не вернулось в норму. хотя трафик снизился.

Код и конфигурация менялись несколько дней назад.
НО если бы дело было в App Engine - проблемы были бы у всех приложений.

Эксалировали на разработку.
Разработчик покопался и увидел что увеличение задержкки коррелирует
с запросами merge_join, которые часто указывают на неоптимальную индексацию.

Настройка индексов помогла бы.
Но как найти что индексировать?

Беглый анализ подозреваемых не выявил.

Подключили Dapper.
Потрейсили HTTP запрос начиная от прокси до формированяи ответа.
И посмотрели вызовы RPC, участвующих в ответе.
Это позволило глянуть какие свойства запрашивались,
и создать по ним индексы.

Параллельно обнаружили, что запросы к статике, тоже выполнялись
медленее чем обычно.

Значит предположение о merge_join было не совсем верным.

Запросы шли через сервер кэширования
и должны быть быстрыми.

Но приложение юзера что-то делало, и SRE не знала что.
При этом не было RPC, чтобы отловить через Dapper.

Забили на решение. Клиент накинул ЦПУ.

Изменение в режиме работы - тоже причина задержек.
Заметили рост кол-ва операций в хранилище данных.
Но рост был небольшим и коротким. Копать дальше не стали.
Навеялись мысли о кэшировании.

Нежелательно хранить в памяти копию конфигурационных данных.
Не заметишь изменений оригинала в хранилище.

Нашли кусок, который проверяет доступ пользователя к данному пути.
И белый список (видимо пользователей) - кэшировался в памяти.

Нашли дефект. - При обращении по конкретному пути - выполнялось создание
белого списка и запись его в хранилище.
При запуске приложений автосканер проверял их на уязвимости,
и в процессе проверки создавал белый список из тысяч объектов
за полчаса.
Это множество избыточных объектов участвовало в проверках доступа при каждом запросе,
что приводило к задержкам ответов.

Важно контролировать расход памяти.

## Как облегчить решение проблем

* Белый ящик и структурированные логи.
Для каждого компонента.

* Предельно понятные и наблюдаемые интерфейсы между компонентами.

UID запросов через всю систему.

Аудит изменений с системой.

# 13 Реагирование в критических ситуациях


...

# 14 Управление в крит ситуациях

## Рекурсивное разделение обязанностей
* Управление. Начальник штаба.формирует команду, распределяет обязанности.
* Опергруппа. Только она вносит изменения в систему. Контактирует с нач. штаба через своего лидера.
* Информирование. Дайджесты о происходящем. Актуальная и достоверная документация об инциденте.
* Планирование. Поддержка опергруппе.
Регистрация ошибок, заказ обеда, передачи задач между исполнителями.
Фиксация расхождений в системе, чтобы можно было вернуть в исходное состояние.


Критерии инцидентов.
 Вам необходимо привлекать вторую команду, чтобы решить проблему.
  Перебой в работе заметили клиенты.
  Проблема не решена даже после нескольких часов сосредоточенной работы.

































































































8 инженеров













































