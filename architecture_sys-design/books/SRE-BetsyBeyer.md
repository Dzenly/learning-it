Site Reliability Engineering, edited by Betsy Beyer,
Chris Jones, Jennifer Petoff, and Niall Richard Murphy (O’Reilly). Copyright 2016
Google, Inc., 978-1-491-92912-4.

https://www.youtube.com/watch?v=q68S0wKy5vE

После появления ПО продукта много сил (40-90%) надо на поддержку и развитие.

# Цикл жизни ПО

* Создание
* Развертывание
* Функционирование
* Обновление
* Вывод из эксплуатации

====

Для разного ПО есть своё понятие "достаточно надежности".
АЭС - одно, интернет - магазин - другое.

====

Site в SRE - легаси.

====

SRE отличается от DevOps тем, что в нем упор идет на надежность.

====

Даже доскональная документация и жирные красные шрифты чего делать
не надо, не гарантируют отсутствие ошибок.
Люди все-равно рано или поздно сделают что-то запрещенное документацией.

====

Учет рисков - ключевой момент в SRE.

====

Надеяться - это плохая стратегия.

Operations (ops) - служба эксплуатации (развертывание, дежурство).

Без автоматизации OPS масштабируется только за счет найма админов.

Цели Ops и Dev противоречат друг другу.

Ops проверяют продукт.
Dev дробят продукт, чтобы проверять
нужно было только измененную часть.

Гугль в SRE команды набирает разработчиков, чтобы они
там по-максимуму все автоматизировали.

Это либо чистые разработчики без навыков админства (40-50%),
либо разработчики с навыками админства (UNIX, Network (OSI L1-L3)). Но всегда разработчики.

SRE разрабатывают приложения для решения сложных задач,
т.е. автоматизируют Ops.
Постоянно дорабатывают и улучшают свою автоматизацию.

**Гуглевский лимит - не больше 50% на операционку.**
Как раз чтобы было время на улучшение автоматизации.
В идеале стремятся к 100% автоматизации.

Как достигать этих < 50% операционки: перекладывать часть на Dev (весь избыток над 50%, вплоть до привлечения разработчиков к дежурствам и переадресации поступающих ошибок менеджерам разработки) или введение в SRE команду людей, на которых нет операционки.

При таком грамотном подходе, при масштабировании системы
SRE команда растет медленнее, чем система.

Ротации между разработчиками и SRE/

Из трудностей: т.к. в обе команды идут программисты -
конкуренция за людей с рынка труда.

**В менеджменте есть свои трудности**: например, остановить
выпуск версий до конца квартала, т.к. исчерпан лимит
времени недоступности сервиса.**

DevOps - обобщение SRE для широкого круга организаций, управленческих структур и персанала.
SRE - имплементация DevOps с расширениями.

# SRE команда отвечает за:

* Доступность
* Время отклика
* Производительность
* ?Эффективность?
* Управление изменениями
* Мониторинг
* Реагирование в аварийных и критических ситуациях
* Планирование производительности

=======

В организации должны быть систематизированы правила и принципы взаимодействия команд SRE с:
* Сопровождаемыми системами
* С командами разработчиков
* С командами тестировщиков
* С пользователями
* И т.д.

========

Когда разработчики участвуют в SRE (во время авралов, например),
они получают непосредственную обратную связь,
чтобы пилить системы, не требующие большой поддержки.

Если появляется больше двух проблем за 8-12 часовую смену,
- уже не хватает времени хорошо изучить проблему
и поправить архитектуру.

Но если в среднем меньше 1 события за дежурство - работа на месте дежурного - пустая трата времени.
? Почему, ведь можно в перерывах пилить свои задачи,
или добивать техдолг по архитектуре?

Нужно писать отчеты с анализом причин произошедшего (постмортемы)
для всех значимых инцидентов.
Особенно для событий, прошедших мимо мониторинга и алертинга.
Расследование должно установить все детали, и выработать план действий по устранению
или более эффективной обработки подобных инцидентов в будущем.
Чтобы ошибки не замалчивались - важно максимально избегать обвинений.

**Суммарный уровень/бюджет ошибок** (error budget)
Избавление от конфликта Dev vs Ops.

Наблюдение: 100% надежность необоснована в большинстве ситуаций (кроме, допустим: АЭС, кардиостимуляторов, систем безопасности автомобилей).

Разница в 99.999% и 100% теряется на фоне случайных факторов, и пользователь
не получает выхлопа от наших потраченных сил для добавления ещё одной 9ки.

# Вопросы к продукту при проектировании:

* Какой показатель доступности удовлетворит пользователей.
* Какие альтернативы имеют пользователи, неудовлетворенные доступностью продукта.
* Как изменится использование юзером продукта при разных уровнях доступности.

======

`ErrorBudget = 1 - PlannedAvailabilityTarget`

Этот бюджет можно тратить например, на
внедрение нового функционала и привлечение новых пользователей.

Можно освобождать error budget с помощью:
* Поэтапного развертывания (phased rollout).
* И экспериментального 1 процента (канареечные деплои ?)

В итоге Ops и Dev договариваются совместно на что потретить этот
error budget.
Т.е. баги и сбои не считаются чем-то плохим, ожидаемы, и входят
в тактические планы.
Исчезает страх, появляется лучший контроль.

======

# Мониторинг

* Почта неэффективна, если недостаточно очевидно что надо делать,
или если бывает что это просто некая инфа и делать с ней ничего не  надо.

* Срочные алерты (*alerts*) - нужно быстро отреагировать, т.к.
либо вот-вот произойдет, либо уже произошло что-то плохое.
* Запросы на действия (*tickets*) - Нужно вмешаться, но не обязательно сейчас, можно в течение нескольких дней.
* Журналирование (*logging*) - Пишется на всякий случай, для возможного последующего анализа для тикетов и алертов.

# Реакция на критические ситуации

MTTF - Mean Time To Faifure - Среднее время безотказной работы
MTTR - Mean Time To Repair - Среднее время восстановления

Ручные вмешательства - долго.
Продумывание всех деталей и **прописывание рекомендаций в инструкцию** - утраивает скорость восстановления системы, по сравнению с импровизациями.

Игры "Колесо неудачи" (Wheel of Misfortune)

============

# Управление изменениями

70% сбоев происходит из-за изменений в работающей системе.

## Нужно юзать автоматизацию для
* Поэтапного развертывания обновлений ПО;
* Быстрого и точного выявления проблем;
* Безопасного отката изменений при ошибках.

Типа убираем человеческий фактор: всякие усталости, невнимательности, замыленности, и т.д.

# Прогнозирование нагрузки и планирование производительности (ресурсов, мощностей)

* Закладывать небольшую избыточность?
* Нужно **как можно точнее** учитывать и естественный постепенный рост, и скачкообразные росты (новый функционал, сезонные эффекты, и т.д.). В том числе и за пределами срока, требуемого для ввода новых мощностей.
* Регулярное НТ, с отслеживанием насколько в каких местах грузится железо.

Команда SRE должна отвечать за все это планирование и за железо
(материально-техническое обеспечение (provisioning)).

# Мат-Тех обеспечение (Provisioning)

Оборудование дорогое. Надо делать все быстро и правильно, но только когда это
правда необходимо.

Наращивание производительности это: введение новых инстансов систем,
площадок размещения инфры, внесение изменений в конфиги, балансеры,
сетевые настройки, и т.д. Плюс проверка что все работает ожидаемо.

# Эфективное использование ресурсов

Коэффициент использования ресурсов влияет на стоимость сервиса.
При каком-то критичном перегрузе - может быть полный останов системы.

==============

# Оборудование

* Машина (комп, возможно виртуальная)
* Сервер - еденица ПО, реализующая сервис.

Сервера не привязаны к машинам. Ресурсы распределяются системой *Borg*.

* *Стойки* (десятки машин)
* *Ряды* стоек.
* Несколько рядов - *кластер*.
* Несколько кластеров - *ЦОД*.
* *Кампус* - Несколько близко расположенных ЦОДов.

Крутая сеть. Jupiter.
B4, OpenFlow.

==============

Для себя: видимо SRE все-таки должно норм владеть аппаратной частью,
чтобы понимать какие нагрузки какое оборудование держит.
Или даже писать софт, которое управляет конфигами оборудования.
Ну и надо понимать, что железо ломается, и не является
чем-то надежным.

==============

# ПО для оборудования

В крупных кластерах сбои оборудования случаются часто.
За год могут сломаться тысячи машин и жестких дисков.
**В общем, железо - расходник, не стоит рассчитывать на его надежность**

* В каждом кампусе есть команда, отвечающая за поддержку оборудования,
и инфраструктуру дата-центра.

## Управление машинами

*Borg* (он породил Кубер, похож на Apache Mesos)
borglet - kublet.

Borg запускает jobs. Это могут быть как постоянные сервисы,
так и процессы пакетной обработки вроде MapReduce.
Могут быть тысячи задач.
Работает примерно как кубер.

IP не статические.
Borg Naming Service, выделяется индекс для задачи.
`/bns/<cluster>/<user>/<job_name>/<job_index>`
И это уже можно преобразовать в `IP:Port`.

## Хранилище

Некая распределенная файловая система, похожая на HDFS или Lustre.

Disk(диски, флеши)->Colossus(FS, replication, cyphering, наследник Google File System)->BigTable (NoSQL, распределенная, разреженная, отказоустойчивая, многомерная БД, индексируется по ключам строк, столбцов и временным метком, каждое значение - произвольный массив байт, поддерживает репликацию)->Spanner (SQL для пользователей, где нужна целостность и согласованность).

Есть ещё всякие blobstore.

## Сеть

OpenFlow
Вместо умных и дорогих роутеров - глупые и дешевые коммутаторы + центральный контроллер, вычисляющий лучшие маршруты.

Bandwidth Enforcer (BwE) управляет доступеной полосой. Максимизирует
среднюю пропускную способность.
Централизованное управление трафиком решает проблемы,
которые плохо решаются распределенной маршрутизацией.

### Global Software Load Balancer, GSLB - три уровня:

* Географическая балансировка нагрузки для DNS запросов.
* Балансировка на уровне пользовательских интерфейсов (YouTobe, GMaps);
* Балансировка на уровне RPC.

Как данные для балансировки используются:
* Символьные имена, список BNS адресов.
* Производительность, доступная на каждой площадке (QPS, queries per second).

# Другое системное ПО

## Сервис блокировок (Chubby)
Предоставляет API, схожий с файловой системой.
Юзает Paxos для обращения к Consensus.
Играет роль при выборе мастера из реплик.
BNS использует Chubby

## Мониторинг и алертинг

Borgmon

* Оповещения о неотложных проблемах
* Информация для оценки как очередное ПО влияет на производительность
* Информация как потребляются ресурсы (допустим, с ростом кол-ва клиентов, или в какие-то сезонные моменты.). Все это - инфа для планирования нагрузки.

# Инфраструктура ПО

Сервисы умеют в многопоточность, и предоставляют HTTP интерфейс,
для сборка диагностической инфы и статистики.
Сервисы общаются по gRPC.

Часто локально вызывают RPCшные функции.
Потом легко отделить сервис.
GSLB - балансирует gRPC нагрузку.

Сервер получает RPC с фронта и отправляет их в бакэнд.
protobuf.

# Среда разработки

Общий репозиторий??
Можно делать МР в чьих-то файлах.
Ревью МР.
Параллельная компиляция.
CL - change list.
Тестирование кусков, на которые эти изменения **прямо или косвенно** могут повлиять.
**push-on-green** - Отправка в прод при успехе.

# Shakespeare: пример сервиса

* Читаем все найденные пока что тексты Шекспира и запихиваем их в Bigtable. Если найдем новые тексты - вызовем процедуру ещё раз.
* Фронт - для связи с пользователями.

## map-reduce
* Mapping: все тексты разбиваются на отдельные слова. Процесс идет параллельно.
* Shuffle - сортировка по словам.
* Reduce - индексы, кортежи вида (слово, список произведений).
Каждый кортеж - строка в Bigtable, ключ - слово.

## Жизненный цикл запроса

GSLB по символьному имени определяет какой IP возвратить пользователю.
Браузер соединяется с Reverse Proxy (Google Front End), он ищет сервис,в данном случае Shakespeare.
Дальше идет RPC на фронтэнд-сервер Shakespeare. И дальше запрос упаковывается в protobuf и идет на бакэнд-сервер. Через BNS, чтобы тоже балансировать нагрузку. Бакэнд запрашивает данные в Bigtable.
Опять protobuf, фронт-сервер, который создает HTML для ответа юзеру.

Все работает быстро и надежно.
Упреждающие методы восстановления при ошибках - (*постепенное отключение функций*.)

## Организация задач и данных

Допустим бэк может обработать 100 QPS.
Допустим, поняли, что пиковая нагрузка будет 3740 QPS.
Значит надо около 35 бэков.

Но во время обновления одна из задач всегда недоступна, и ещё можно учесть вероятность сбоев.
Поэтому добавим + 2 бэка.

Дальше можно поанализировать нагрузку по регионам, и раскидать
приложение по датацентрам этих регионов соответственно.
N+2 для каждого региона.
Для устойчивости - можно учесть межрегиональные перебросы.
Для дешевизны пренебречь N+2, оставив N+1 для малонагруженных регионов.

Bigtable дублируется в каждом регионе.
Консистентность прямо сию секунду не нужна.

=============

# Принципы

* Оценка рисков
* Управление рисками
* Использование лимита недоступности (error budget)

**SLO** Service Level Objectives

=============

# Приручаем риски

* Надежность не бесплатна. Каждая новая девятка может стоить в 100 раз больше предыдущей.
* Систему не имеет смысл делать сильно надежней, чем компоненты, через которые юзер работает с системой.

Цены:

* Машины. Резервные для переключения при сбоях или при обслуживании. А также хранящие контрольные суммы для обеспечения целостности.
* Упущенные возможности, когда разработчиков перебрасывают в SRE.

Строим графици стоимость, прибыль, уровень надежности, риски.

*Незапланированные отключения* как объективная метрика надежности.

`Availability = Uptime / (Uptime + Downtime).`

99.99 - 53 минуты простоя в год.

Но вообще **кол-во успешных запросов** за какой-то отрезок времени как метрика показательней.

`Availability = Successful requests / total requests.`

Допустим, Потеря 250 запросов из 2.5 млн запросов в день - 99.99%.
Есть разные типы запросов, они сильно важные, другие - нет. Так что это не 100% объективная метрика, но она достаточно хороша.

Запросы могут быть и всякие внутренние. Мож даже и не связанные с юзерскими запросами (а мож и связанные).

Доступность гугль замеряет раз в квартал. А производительность
еженедельно или даже ежедневно.

## Рискоустойчивость сервисов

Иногда это критически важно. И тогда понятие рисков включено в характеристики продукта.

Риски оценить не всегда просто.
Инфраструктурные сервисы (системы хранения или уровень кэширования для HTTP) могут не иметь определенного владельца.

### Пользовательские сервисы

Часто имеют команды и продакт овнера (менеджера).
Если команды нет, требования к доступности выполняют
люди, создающие систему, даже если они об этом и не знают.

#### Факторы при оценке рисков

* Какого уровня доступности надо достичь.
* Как различные типы сбоев влияют на сервис.
* Как мы можем менять стоимость сервиса, чтобы позиционировать его на кривой зависимости рисков.
* Какие другие показатели сервиса нужно иметь в виду.

#### Целевой уровень доступности

* Какого уровня доступности будут ожидать клиенты.
* Связан ли сервис непосредственно с прибылью (нашей или пользователей).
* Сервис платный или бесплатный?
* *Какие есть конкуренты, какой уровень сервиса у них.*
* *Сервис для предприятий или для конечных пользователей?*

Google Apps for Work - 99.9%.
+ Более высокий уровень внутренней доступности.
+ Контракт со штрафами, если не выполнили уровень.

А вот когда Google купил YouTube, он поставил уровень надежности поменьше, **чтобы развиваться быстрее**.

#### Типы сбоев

* Что лучше несколько мелких сбоев или один большой?
* Есть полные, есть частичные сбои. Например, в профайле все работает, кроме фоток.
* **Есть сбои, когда лучше полностью отключить сервис**. Например, когда компрометируются данные пользователей.
* Некоторые сервисы используются в определенное время дня, и их можно отключать в другое время.

#### Стоимость

* Что нам стоит добавить ещё 9-ку?
* Сколько мы получим прибыли из-за этого?

Если нет однозначной зависимости прибыли от надежности.
Можно взять за основу надежность сервисов, через которые используют твой сервис.
Например, надежность интернет провайдера пользователя.
Разные инет провайдеры это 99 - 99.99%.

#### Другие показатели сервисов

* Задержка (иногда она даже желательна, например, для показа рекламы)


## Определение рискоустойчивости инфраструктурных сервисов

Отличаются от требований к потребительским продуктам.
Т.к. имеют несколько клиентов, чьи потребности часто не совпадают.

Где-то важна *надежность*, где-то *пропускная способность*,
где-то *латентность*.

* Какое состояние очереди запросов желательно для пользователей каждого типа?

### Типы отказов

* Те, которым нужна низкая латентность - хотя видеть очереди пустыми.
* Те, кому нужна пропускная способность - хотят, чтобы очереди были не пустыми.

### Стоимость

Можно разделить инфру и ввести несколько независимых уровней обслуживания.

Короткие очереди - за счет доп. ресурсов.
Конкуренция между потребителями уменьшается, очереди разгружаются.

Но в 2-10 раз дешевле настроить кластер на максимальную пропускную способность.

Можно раскидать критичные данные и не очень по хранилищам с разными характеристиками и разной стоимостью содержания.

На одном и том же железе и ПО можно настраивать разные классы систем,
за счет конфигов.

### Пример: фронтенд-архитектура

Фронтэнды (реверс прокси, балансировщики) делают максимально надежными. Т.к. тут есть риск полностью потеряь запрос.


## Обоснования критерия суммарного уровня ошибок (errors budget)

Природа конфликта Dev vs Ops.
Разработчиков оценивают по тому, сколько фич они запилили.
Ops оценивают по надежности систем.

* *Устойчивость* к сбоям. Слабая устойчивость а нежданчикам -
будет дырявое в плане безопасности и нестабильное приложение.
Если устойчивость будет сильной, приложение никто не захочет
использовать. *Почему??* Дорого?? Долго, конкуренты опередят?
* *Тестирование*. Мало тестишь - ошибки, разного уровня серьезности.
Долго тестишь - теряешь рынок.
* *Частота релизов*. Каждое обновление у клиента - риск.
* *Продолжительность тестирования и размер выборки*.
Канареечные версии. Как долго ждать и как много юзеров переводить на канарейку.

Во многих компаниях и продуктах это все выбирается необъективно.
Какие-то исторически сложившиеся компромисы.
Лучше стараться оценивать объективно на разных количественных метриках.

## Формируем error budget

Команды его совместно определяют на квартал.
Основываясь на SLO.

Гугль делает так:

* Менеджер продукта определяет SLO. Задавая время uptime в течение квартала.
* Система мониторинга (нейтральная третья сторона) замеряет текущий uptime.
* Разница и есть запас (бюджет) на оставшуюся часть квартала.
* Если есть запас - можно продолжать выпуск обновлений.

Например, если какая-то проблема съедает 0.0002 аптайма, а целевой 0.001. То бюджет ошибок эта проблема выгребает на 20%.

## Преимущества

Поиск баланса между инновациями и надежностью. Вместе. И Ops и Dev.
Если SLO не достигается - приостанавливаем выпуски, дорабатываем продукт. Можно даже откатить релиз.

И тут разработчики подумают стоит ли выпускать релиз, который
может съесть весь бюджет и остановить разработку.
Это если у SRE команды есть право останавливать релизы.

Если обе команды понимают что дело в железе - они сообща
решают приостановить релизы.

SLO не обязан быть константой, при каких-то условиях его можно подправлять.

=================

## Целевой уровень качества обслуживания

### SLI - service level indicators (показатели)
Четко определенное числовое значение конкретной характеристики
обслуживания.
* Время отклика
* Уровень и частота ошибок (процент успешных запросов от всех, даже сформированных некорректно)
* Throughput (QPS)
Берется окно измерения, данные усредняются, рисуются процентили.

Не всегда можно напрямую измерить что-то.
Мы часто знаем время обработки запроса на сервере, но вот задержку у клиента померить сложнее.

* *Доступность* (availability) Доля успешных запросов из тех, что сформированы корректно. Также называется *yield* (выработка, отдача, урожай).
* *Durability* - долговечность. Вероятность что данные не потеряются
за какое-то время.

Две девятки - 99. Пять девяток - 99.999.
Google Compute Engine - 3.5 девяток. Т.е. 99.95%

### SLO

SLI <= Target
lower_bound <= SLI <= upper_bound

I.e. для Shakespeare - средняя латентность поиска должна быть меньше 0.1 сек.

Значит надо пилить фронт с низкой латентностью, или улучшать оборудование.

Выбор SLO сложен. Например, QPS определяется субъективными
ощущениями пользователей.

Есть статья Speed Matters, где проведены исследования как я понял по тому, как скорость влияет на удержании юзеров.

И латентность и QPS могут ухудшаться после определенного предела нагрузки.

Юзеры могут считать систему и более надежной чем она есть и менее надежной.

Например, кто-то там в своих сервисах завязался на Chubby, считая его 100% надежным, и был неприятно удивлен во время их обслуживании.
Иногда чтобы приучать юзеров создают **искусственные сбои**.

### SLA

Явный или нет контракт с юзерами, **включающий последствия**
при соответствии или несоответствии SLO.

**Можно создавать и без SRE**

Google Search - сервис без SLA.

=================

## Показатели на практике

Не надо в SLI пихать все, что есть в мониторинге.
Надо выбрать действительно важные параметры.

* Для *пользовательских сервисов* обычно важны *доступность*, *латентность*, *пропускная способность*.
* Для *систем хранения* - *латентность*, *доступность*, *долговечность*.
* Для *систем обработки больших объемов данных* (Big Data Systems) (pipeline processing) -
*thoughput* and *end-to-end latency* (from ingestion to completion).
Некоторые конвееры могут иметь целевые показаели латентности на индивидуальных этапах).
* *correctness* - For all systems. Правильные ли данные возвращены,
был ли корректен выполненный анализ.
Правда он часто - свойство данных а не системы, и не входит в зону ответственности SRE.

### Сбор показателей

Borgmon или Prometheus.
Журналы приложений.
Иногда данные можно собирать на  **стороне клиента**.
Например скорость обработки JS или метрики по загрузке CDN.

### Агрегация

Часто сырые первичные данные агрегируются. Это нужно делать осторожно.

Если у вас есть выброс в обработке запроса, который сгладился
т.к. вы усредняете по минуте, вы проглядите проблему.

**Лучше смотреть распределения** по персентилям.

50-й персентиль - медиана, типичные ситуации.

Чем выше разброс значений, тем больше влияет на пользователя
хвост распределения. Эффект усугубляется при высоких нагрузках
из-за поведения очередей.

**SRE Можно ориентироваться на какой-нибудь 99.9 процентиль.**

О статических заблуждениях: среднее для IT не то же самое как для физики. Среднее арифметическое может сильно отличаться от медианы.
И закон распределения не факто что будет нормальным.
Это всё нужно учитывать в автоматизации. Например если автотулза
рестартит сервис, она должна полагаться не на интуитивные распределения, а на фактические.

### Стандартизация показателей (SLI)

Можно стандартизовать некие общие показатели, чтобы не думать над
ними каждый раз заново.
Тогда останется думать только над специфическими показателями.

* Интервал агрегации: среднее значение по минуте.
* Регион агрегирования: все задачи в кластере.
* Частота измерений: каждые 10 сек.
* Какие запросы включаем в выборку: GET, полученные мониторингом методом черного ящика.
* Способ получения данных: Получены путем наблюдения. Измерения на стороне сервера.
* Задержка при доступе к данным: Время получения последнего байта.

## Целевые показатели на практике

В первую очередь отталкиваться от того, что важно клиентам.
А не от того, что мы можем измерить.
Т.е. лучше ставить цели, а потом думать как замерить показатели для них.

### Определение целей

Нужна инфа как измерять SLO и условия их действительности.
Например:

* 99% (в среднем за минуту) вызовов RPC Get будут завершены
менее чем за 0.1 сек. (По всем бэкам).
* 99% RPC Get будут завершены менее чем за 0.1 сек. Остальные параметры эта строка берет из SLI defaults.

Если важна форма кривых производительности, можно указать несколько
целевых значений:

* 90% RPC Get будут завершены менее чем за 1ms.
* 99% RPC Get - 10ms
* 99.9% RPC Get - 100ms

Если есть юзеры с разной загруженностью (вроде конвейера обработки
массивов данных), для которых важна throughput и интерактивный клиент, для которого важно время отклика, то для каждого вида
нагрузки лучше задать свои SLI

* 95% RPC Set, для которых важна throughtput, < 1 sec.
* 99% RPC Set, для которых важна latency и которые имеют нагрузку
меньше 1Кб, - будут завершены < 10ms.

SLO не должны соблюдаться 100% времени.
Лучше установить уровень допускаемого несоответствия SLO,
и регулярно его трекать (ежедневно, еженедельно),
тогда можно увидеть, что что-то потихоньку начинает идти не так,
и заранее принимать меры.

+ Ежемесячный или ежеквартальный отчет по использованию error budget.

### Выбор целевых показателей

Что надо учитывать:

* Интересы продукта и бизнеса.
* Ограничения кадрового состава.
* Сроками выхода на рынок.
* Доступность аппаратных средств.
* Финансы.

#### Уроки гугла

* Не выбирать цель, основываясь на текущей производительности.
Лучше безотносительно текущих подумать какие они должны быть.
Возможно где-то можно сэкономить.
* Не усложняйте агрегацию. Тут можно потерять важные данные.
* Не гонитесь за абсолютом. Good enough for users.
* Минимизируйте кол-во SLO до тех, которых достаточно
для получения полной информации о характеристиках системы.
Обосновывайте необходимость SLO. Если среди команд нет согласия,
что данный SLO нужен. То может лучше его и не вводить. Да и возможно
SRE тут не нужно.
* Идеал может подождать. Лучше сначала ориентироваться
на реально доступный уровень, чем тужиться потом.

SLO должны стать главным ориентиром при приоритезации задач SRE
и даже разработчиков.
Чем лучше продуманы SLO, - тем эффективней тратится время Dev и SRE,
и тем более доволен клиент.

### SLO как элемент управления

* Измеряйте SLI
* Сравнивайте с SLO, решайте нужно ли что-то предпринять.
* Решайте что конкретнопредпринять.
* Действуйте.

Например, идет рост времени отклика. Можно временно повысить ресурсы,
и разбираться почему отклик растет.

### Опубликованные SLO формируют ожидания пользователей

Соответственно пользователи имеют больше инфы чтобы выбрать или
не выбрать ваш сервис.
Например, если доступность низкая - это норм для хранения не сильно нужных архивов.

#### Тактики

* Запас прочности. Более жесткие внутренние SLO.
Тогда будет больше времени на разбор проблем и больше маневров занести в продукт что-то тяжелое (но м.б. более дешевое).
* Избегать перевыполнения. Пример с Chubby. Не приучайте юзеров
завязываться на крутые SLO. Время от времени имитируйте отключения.
**Или можно искусственно ограничивать QPS до SLO-шных.**
Или проектировать систему так, чтобы она работала с одинаковой скоростью при разных нагрузках. Т.е. допустим довыделяла себе
только сколько надо ресурсов при больших нагрузках, и возвращала
их обратно при снижении нагрузок.

### Соглашения на практике

Когда думаем над SLO с овнерами и юристами, роль SRE - донести реалистичность выполнения разных SLO.
Чем шире круг пользователей, тем труднее изменить или удалить SLA,
которые оказались трудновыполнимыми или необоснованными.

===========================







